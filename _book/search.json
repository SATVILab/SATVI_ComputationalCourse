[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SATVI Computational Course",
    "section": "",
    "text": "Preface\nThis is a session guide book for the SATVI Computational Course.\nThis is a version-controlled living document that will be updated as needed as the course progresses. All changes are tracked using git.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Instructor contacts\nThe full curriculum can be found on the “Syllabus” page.\nHappy coding!\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#instructor-contacts",
    "href": "intro.html#instructor-contacts",
    "title": "1  Introduction",
    "section": "",
    "text": "Carly Young-Baile: carly.young-bailie@uct.ac.za\n\n    Monika Looney: monika.looney@uct.ac.za\n\n    Miguel Rodo: miguel.rodo@uct.ac.za\n\n    Simon Mendelsohn: simon.mendelsohn@uct.ac.za\n\n    Munyaradzi Musvosvi: munyaradzi.musvosvi@uct.ac.za\n\n    Denis Awany: denis.awany@uct.ac.za\n\n\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "2  Syllabus",
    "section": "",
    "text": "2.1 Description\nThis page serves as a syllabus for the SATVI Computational Course. Details for each session can be found on their dedicated page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#module-1-intro-to-r-and-marcus-training-course",
    "href": "syllabus.html#module-1-intro-to-r-and-marcus-training-course",
    "title": "2  Syllabus",
    "section": "2.2 Module 1: Intro to R and MaRcus Training Course",
    "text": "2.2 Module 1: Intro to R and MaRcus Training Course\n\n2.2.1 Session 1: Intro to R and swirl\nTopic: Introduction to R language and environments, RStudio, and swirl self-teaching tools.\nInstructor: Monika Looney: monika.looney@uct.ac.za\nDate: 05 MAR 2024\nTime: 10h30 - 11h30\nLocation: Lekgotla 4A and 4B\nHomework: Complete swirl “R Programming” interactive learning sessions at own pace.\n\n\n2.2.2 Session 2: MaRcus Training Course lesson 1\nTopic: Importing data into R environment and basic visualizations with ggplot2\nInstructor: Carly Young-Baile: carly.young-bailie@uct.ac.za\nDate: 19 MAR 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: See assignment from MaRcus R Training Course\n\n\n2.2.3 Session 3: MaRcus Training Course lesson 2\nTopic: Creating histograms and statistical summaries; combining and exporting plots\nInstructor: Carly Young-Baile: carly.young-bailie@uct.ac.za\nDate: 26 MAR 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: See assignment from MaRcus R Training Course\n\n\n2.2.4 Session 4: MaRcus Training Course lesson 3\nTopic: Basic data transformation using dplyr\nInstructor: Carly Young-Baile: carly.young-bailie@uct.ac.za\nDate: 02 APR 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: See assignment from MaRcus R Training Course\nNote - MaRcus Training Course lesson 4 was skipped as it covers R Markdown which will be replaced by a session on Quarto later.\n\n\n2.2.5 Session 5: MaRcus Training Course lesson 5\nTopic: Continuation of data transformation using dplyr and data wrangling\nInstructor: Carly Young-Baile: carly.young-bailie@uct.ac.za\nDate: 09 APR 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: See assignment from MaRcus R Training Course\n\n\n2.2.6 Session 6: MaRcus Training Course lesson 6\nTopic: Clean up data using tidyr\nInstructor: Carly Young-Baile: carly.young-bailie@uct.ac.za\nDate: 30 APR 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: See assignment from MaRcus R Training Course\n\n\n2.2.7 Session 7: MaRcus Training Course lesson 7\nTopic: Manipulating strings with stringr and intro to regular expressions\nInstructor: Carly Young-Baile: carly.young-bailie@uct.ac.za\nDate: 07 MAY 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: See assignment from MaRcus R Training Course\n\n\n2.2.8 Session 8: Exporting data from R\nTopic: Exporting data and plots from R in different formats including csv, pdf, and jpeg\nInstructor: Monika Looney: monika.looney@uct.ac.za\nDate: 21 MAY 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: Load and export data frame to csv and excel. Save plots as pdf and jpeg.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#module-2-quarto-github-and-guis",
    "href": "syllabus.html#module-2-quarto-github-and-guis",
    "title": "2  Syllabus",
    "section": "2.3 Module 2: Quarto, GitHub, and GUIs",
    "text": "2.3 Module 2: Quarto, GitHub, and GUIs\n\n2.3.1 Intro to Quarto\nTopic: Intro to technical publishing using Quarto\nInstructor: Monika Looney: monika.looney@uct.ac.za\nDate: 04 JUN 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: Initialize a Quarto project for your own study; Make GitHub account, access SATVILab GitHub, and download GitHub Desktop\n\n\n2.3.2 Intro to GitHub\nTopic: Intro to version control using Git and GitHub\nInstructor: Monika Looney: monika.looney@uct.ac.za\nDate: 18 JUN 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: Set up a version controlled project; Download VS Code\n\n\n2.3.3 Intro to VS Code\nTopic: Intro to VS Code as an alternative GUI to RStudio and git-aware terminals\nInstructor: Monika Looney: monika.looney@uct.ac.za\nDate: 02 JUL 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework: Write a script in the VS Code GUI",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#module-3-statistics",
    "href": "syllabus.html#module-3-statistics",
    "title": "2  Syllabus",
    "section": "2.4 Module 3: Statistics",
    "text": "2.4 Module 3: Statistics\n\n2.4.1 Basic statistical tests\nTopic: Computing commonly needed statistics and confidence intervals in R\nInstructor: Miguel Rodo: miguel.rodo@uct.ac.za\nDate: 23 JUL 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:\n\n\n2.4.2 Correlations\nTopic: Computing correlation metrics in R\nInstructor: Miguel Rodo: miguel.rodo@uct.ac.za\nDate: 30 JUL 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#module-4-commonly-needed-analyses-for-immunology",
    "href": "syllabus.html#module-4-commonly-needed-analyses-for-immunology",
    "title": "2  Syllabus",
    "section": "2.5 Module 4: Commonly needed analyses for Immunology",
    "text": "2.5 Module 4: Commonly needed analyses for Immunology\n\n2.5.1 Heatmaps\nTopic: Plotting and manipulating heatmaps\nInstructor: Monika Looney: monika.looney@uct.ac.za\nDate: 27 AUG 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:\n\n\n2.5.2 Dimensionality reduction\nTopic: Understanding and conducting dimensionality reduction using PCA and UMAP\nInstructor: Monika Looney: monika.looney@uct.ac.za\nDate: 03 SEP 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:\n\n\n2.5.3 Receiver operating characteristic (ROC) curves\nTopic: Understanding and computing ROC curves\nInstructor: Simon Mendelsohn: simon.mendelsohn@uct.ac.za\nDate: 17 SEP 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:\n\n\n2.5.4 Processing gated cytometry data\nTopic: Processing gated cytometry count and frequency data, exported from FlowJo\nInstructor: Miguel Rodo: miguel.rodo@uct.ac.za\nDate: 01 OCT 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:\n\n\n2.5.5 Basic flow cytometry analysis\nTopic: Plotting background subtracted frequencies and MFIs from flow cytometry data\nInstructor: Munyaradzi Musvosvi: munyaradzi.musvosvi@uct.ac.za\nDate: 15 OCT 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:\n\n\n2.5.6 Automatic gating\nTopic: Generating inputs for and carrying out automated gating for flow cytometry data\nInstructor: Munyaradzi Musvosvi: munyaradzi.musvosvi@uct.ac.za\nDate: 29 OCT 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#module-5-other-coding-languages",
    "href": "syllabus.html#module-5-other-coding-languages",
    "title": "2  Syllabus",
    "section": "2.6 Module 5: Other coding languages",
    "text": "2.6 Module 5: Other coding languages\n\n2.6.1 Intro to Python\nTopic: Basics of using python and applications for computational immunology\nInstructor: Denis Awany: denis.awany@uct.ac.za\nDate: 12 NOV 2024\nTime: 11h00 - 12h00\nLocation: Lekgotla 4A and 4B\nHomework:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "installations.html",
    "href": "installations.html",
    "title": "3  Installations",
    "section": "",
    "text": "3.1 Description\nThis document provides installation guides for basic programming tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#r",
    "href": "installations.html#r",
    "title": "3  Installations",
    "section": "3.2 R",
    "text": "3.2 R\nR is a commonly used coding language for computational biologists and immunologists. Many software packages and analysis pipelines depend on R. R is also a computational environment used for computing and generating graphics.\nTo install R for Windows or Mac, follow the instructions provided by The Comprehensive R Archive Network (CRAN) found here: https://cran.r-project.org/\nIt is recommended to download the precompiled binary distribution appropriate for your machine.\nTo learn more about R, read the following introduction provided by CRAN: https://www.r-project.org/about.html",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#r-studio",
    "href": "installations.html#r-studio",
    "title": "3  Installations",
    "section": "3.3 R Studio",
    "text": "3.3 R Studio\nRStudio is an integrated development environment (IDE) based on R. It provides a user-friendly option for building code and can incorporate multiple languages including python, which is also commonly used by computational immunologists.\nTo donwload and install RStudio Desktop, follow this link and the provided instructions: https://posit.co/download/rstudio-desktop/#download",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#github-desktop",
    "href": "installations.html#github-desktop",
    "title": "3  Installations",
    "section": "3.4 GitHub Desktop",
    "text": "3.4 GitHub Desktop\nGitHub Desktop is a desktop application that interfaces with version-controlled code, GitHub, and other Git services. It provides a user friendly GUI where you can review changes made to code and perform Git commands. It is open source and free to use.\nFirst sign up for a GitHub account at https://github.com\nNow download and install GitHub Desktop, follow this link and the provided instructions: https://desktop.github.com. Sign in with your GitHub account login.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "installations.html#visual-studio-code-vs-code",
    "href": "installations.html#visual-studio-code-vs-code",
    "title": "3  Installations",
    "section": "3.5 Visual Studio Code (VS Code)",
    "text": "3.5 Visual Studio Code (VS Code)\nVS Code is a text and code editor commonly used by developers. It can be used as an alternative for RStudio and supports multiple coding languages and various extensions for debugging and version control.\nTo download and install VS Code, follow this link and the provided instructions: https://code.visualstudio.com/download",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installations</span>"
    ]
  },
  {
    "objectID": "swirl.html",
    "href": "swirl.html",
    "title": "4  swirl",
    "section": "",
    "text": "4.1 Description\nswirl is an interactive R package that helps you self-teach the basics of R. It is run from directly from the R console.\nThis session guide follows the instructions provided by swirl. Visit the following link to access the full tutorial.\nYou can also find the full swirl course tutorial on GitHub.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#install-swirl",
    "href": "swirl.html#install-swirl",
    "title": "4  swirl",
    "section": "4.2 Install swirl",
    "text": "4.2 Install swirl\nswirl requires R 3.1.0 or later installed on your computer. It is also recommended that you have RStudio installed which will provide a user-friendly environment to work with.\nFor instructions on how to install R and RStudio, visit the Installations session guide page.\nOnce you have downloaded R and RStudio, perform the following steps:\n1. Open RStudio. \n2. In the RStudio console, type the following where you see the command prompt &gt; :\n\n    install.packages(\"swirl\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#initialize-swirl",
    "href": "swirl.html#initialize-swirl",
    "title": "4  swirl",
    "section": "4.3 Initialize swirl",
    "text": "4.3 Initialize swirl\nWhenever you want to run swirl, you must load and initialize the package.\n1. In the console, type the following:\n\n    library(\"swirl\")\n    swirl()\n\n2. Follow any prompts that come up in the console. i.e. if swirl asks \"What shall I call you?\", type your name and hit enter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#install-an-interactive-course",
    "href": "swirl.html#install-an-interactive-course",
    "title": "4  swirl",
    "section": "4.4 Install an interactive course",
    "text": "4.4 Install an interactive course\nThe first time you initialize swirl, you will need to install a course.\nFor the SATVI Computational Course, we recommend that those who are new to coding start with “R Programming”. This course will cover the basics of programming in R.\nThere are many courses to choose from, so those who are more advanced may opt for an intermediate or advanced course to work through in their own time. A repository with all available swirl courses can be found here: https://github.com/swirldev/swirl_courses#swirl-courses.\nThere is also an expansive swirl Network that expands further on open sourse interactive R lessons. You can access the Network and associated courses or become a swirl course author here: https://swirlstats.com/scn/\nTo install a course that is not part of the swirl course repository, type the following into the console:\n    ?InstallCourses",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#run-swirl",
    "href": "swirl.html#run-swirl",
    "title": "4  swirl",
    "section": "4.5 Run swirl",
    "text": "4.5 Run swirl\nFor now, we will assume that we are starting with the basics and have chosen to install the “R Programming” course.\nTo run the interactive lessons:\nSelect a new lesson. The R Programming course offers 14 different short interactive lessons. Go through each one in order as the information from earlier lessons is required in later lessons.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#exit-swirl",
    "href": "swirl.html#exit-swirl",
    "title": "4  swirl",
    "section": "4.6 Exit swirl",
    "text": "4.6 Exit swirl\nIf at any time you need to exit a swirl lesson before it is complete, simply press the Esc key.\nIf you need to exit from a prompt, exit and save your work by typing: bye()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#interactive-commands",
    "href": "swirl.html#interactive-commands",
    "title": "4  swirl",
    "section": "4.7 Interactive commands",
    "text": "4.7 Interactive commands\nWhile you are working in swirl, you may find that you want to skip a section that you are already comfortable with, or to work more on the current topic outside of an interactive session.\nBelow are some helpful commands for getting the most out of your swirl sessions:\nFrom the R prompt (&gt;):\nTo skip the current question: skip()\n\nTo experiment with R on your own without swirl interaction: play()\n\nTo re-initiate swirl interaction after playing: nxt()\n\nTo exit and save: bye()\n\nTo return to swirl's main menu: main()\n\nTo display these command options: info()\nIf you see a swirl output followed by … press Enter to continue.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#homework",
    "href": "swirl.html#homework",
    "title": "4  swirl",
    "section": "4.8 Homework",
    "text": "4.8 Homework\nAs beginners, regular practice is critical! It is recommended that you go through one or two lessons daily to improve and retain these fundamentals.\nOver the next week, in your own time, complete the 14 short interactive lessons from the “R Programming” swirl course.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "swirl.html#faq",
    "href": "swirl.html#faq",
    "title": "4  swirl",
    "section": "4.9 FAQ",
    "text": "4.9 FAQ\nQ1: Can funcitons learned in swirl be applied when writing my own R scripts?\nA: Absolutely! The functions that you use in swirl are all base R functions that can be used in any R script.\nQ2: If I need to use an R package, do I need to install the package each time I start a new session?\nA: Nope! Once a package is installed, you do not have to re-install when you open a new R session. Simply load the package from the library. For example, if I want to use ggplot2 and I have installed it before, load the package using &gt;library(ggplot2). However, if you are running your script on a different machine that does not have the package installed (and are not working from a container - we'll get to this later in the course), you will need to install the package on that machine before loading it from the library.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>swirl</span>"
    ]
  },
  {
    "objectID": "MarcusRTraining.html",
    "href": "MarcusRTraining.html",
    "title": "5  MaRcus R Training",
    "section": "",
    "text": "5.1 Description\nThe Marcus R Training program was developed by Hasse Walum of Emory University. The program will cover the following:\nRather than reinventing what is covered in the Marcus R Training program, we have been granted permission to use the materials for our SATVI Computational Course.\nOver the next 6 weeks, we will refer to the Marcus R Training materials for our sessions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>MaRcus R Training</span>"
    ]
  },
  {
    "objectID": "MarcusRTraining.html#description",
    "href": "MarcusRTraining.html#description",
    "title": "5  MaRcus R Training",
    "section": "",
    "text": "Importing data\nBasic data visualization\nExporting and saving plots\nData transformation\nR Markdown basics\nSummarizing data\nString manipulation and data joining",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>MaRcus R Training</span>"
    ]
  },
  {
    "objectID": "MarcusRTraining.html#content-access",
    "href": "MarcusRTraining.html#content-access",
    "title": "5  MaRcus R Training",
    "section": "5.2 Content access",
    "text": "5.2 Content access\nThe course and all associated resources are available at:\nhttps://haswal.github.io/MaRcus/index.html",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>MaRcus R Training</span>"
    ]
  },
  {
    "objectID": "MarcusRTraining.html#homework",
    "href": "MarcusRTraining.html#homework",
    "title": "5  MaRcus R Training",
    "section": "5.3 Homework",
    "text": "5.3 Homework\nPlease refer to the MaRcus R Training Program session guides to access your homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>MaRcus R Training</span>"
    ]
  },
  {
    "objectID": "MarcusRTraining.html#faq",
    "href": "MarcusRTraining.html#faq",
    "title": "5  MaRcus R Training",
    "section": "5.4 FAQ",
    "text": "5.4 FAQ\n\n5.4.0.1 Session 1\nQ1: What are the best ways to set your working directory?\nA: There are a few ways to do this:\n\n    1. If you are using Mac, you can navigate to the directory you would like to work in using Finder. Right click the directory and hold down the \"option\" key. You will see an option pop up that reads \"Copy path/to/directory as Pathname\". Select this option to copy the pathname, then paste it into R using setwd(). This option is generally recommended. \n\n    2. You can also set the working directory using point and click in RStudio. To do so, navigte to the \"Session\" tab in RStudio, then to \"Set Working Directory\". Follow the options to set the working directory as desired. \n\n    3. A note about setting working directories in scripts. It is good practice to avoid using absolute file paths in your scripts, including setwd(). This means if you have your data on your PC under the filepath \"User/Documents/ProjectA/analysis/data\", for example, as much as possible, avoid writing this complete path into your code. This becomes problematic if you ever need to run your code on a different machine or HPC, or if a collaborator needs to run your code and has a different file structure. R will not be able to find your data if it is not stored in exactly the same place on every machine. A better option is to set your working directory using the R console (do not setwd() in the saved script). In the console, set your working directory to whatever folder contains your generic subdirectories (i.e. \"data\", \"output\"), in this example, the \"analysis\" folder. Then when you run your script, all you need to include when you specify a filepath is \"data\" or \"output\". So, if you ever run the script on a different machine, the user can store the relevant files anywhere they want, as long as they have subdirectories \"data\" and \"output\". \nQ2: When generating a plot using ggplot2, does the name used in the script for the row or column we want to plot have to match the col or rowname of the associated dataframe exactly?\nA: Yes. The names must match exactly because R searches the dataframe for col or rownames as you specify them. If you have a typo or a lower vs uppercase, it won't find that row or column in the dataframe. Also be aware of any col or rownames that include spaces. Spaces in col and rownames can confuse R into thinking you are indicating more than one name. To avoid this, if you need to specify a col or rowname that includes a space, you must place single back ticks around the full name to indicate to R that this is one complete col or rowname. For example, if you have a column called Sample Name, you must write it into your script as 'Sample Name' for ggplot2 to work properly. \nQ3: What is the difference between facet_wrap() and facet_grid()?\nA: Both are options that can be applied to ggplot2. facet_wrap() wraps a 1d sequence of panels into 2d. It is best used if you want to display data in a rectangular arrangement. It is used when you want to plot one variable. facet_grid() will create a panel matrix defined by numbers of rows and columns. It is ideal for plotting multiple variables that should be visualized separately for the same data. For example, if you want to plot time vs. weight, and time vs. height, you can use facet_grid() to plot both comparisons in a grid layout. Including a ~ either before or after each variable will change the orientation of variables plotted in the grid (i.e. the variable placed before the ~ will be plotted on the row, the variable placed after the ~ will be plotted on the column).\nQ4: When should I specify aes globally vs. locally?\nA: In general, specify aes in mapping (global) so that the specifications are applied to all of the data. If you need something more fine-tuned, you can write it into geom_point (lobal), where it will be applied only to that geom_point. \nQ5: What are HEX codes?\nA: HEX codes are unique alphanumeric codes assigned to specific colors. They can be used to assign specific colors to your data points when you plot in R. There are also pre-defined color schemes (i.e. printer-friendly, colorblind safe) that you can select. To find more information on HEX codes or colorshemes, visit [https://colorbrewer2.org](https://colorbrewer2.org)\nQ6: What are your recommendations for using Chat GPT for help with coding?\nA: Chat GPT is a quickly growing tool used by coders. It can be very helpful for designing / troubleshooting code. We recommend using Chat GPT when you are stuck before spending hours sifting through other online sites. Information given by Chat GPT can and should be verified with the original sources. We caution against allowing Chat GPT to do your work for you! It is best to become proficient and understand coding languages independently and using Chat GPT as an assistant. Relying on it entirely will ultimately be to your detriment. When you do use Chat GPT, read the descriptions and explanations it gives and make you understand thoroughly so you can use it yourself next time. If you do need help in a hurry, you can ask Chat GPT to keep responses brief by typing \"No yapping.\" Always say please and thank you to Chat GPT for offering such superb assistance!\n\n\n5.4.0.2 Session 2\nQ1: What is the difference between top and bottom windows in R Studio?\nA: It can help to think of this an analogy: In R Studio, the top left (script) is your recipe, bottom left (console) is your kitchen, top right (environment) is your pantry. The script window shows your saved scripts that you can use each time you need to run the analysis. You can make edits and save just like you might for your favourite recipe. The console is where you do the work. As you run lines of your script, you will see that in the console below, each line is copied into the console and run in real time. Here you can see the script running and any warnings and outputs. The environment, like a pantry, is where all of the necessary \"ingredients\" to run your script are kept. Just as you would before you start cooking, as you go through your script, you will load necessary items (i.e. data / metadata) into your environment so that when the recipe calls for it, it is there and ready to go. The final bottom right window serves a few functions. It can show file paths, plot outputs, packages etc. We encourage everyone to familiarize themselves with each window to better understand their functionalities. Happy cooking! I mean coding. \nQ2: Can you plot confidence intervals automatically using geom_errorbar or do you have to calculate them separately first?\nA: Confidence intervals should be calculated separately. \n\n\n5.4.0.3 Session 3\nQ1: How can you save the contents of the R console when I finish a session?\nA1: You can save the contents of the base R console using the 'sink()' function. Here you will specify the file path where you want the file to be saved. sink() will then save the console contents as a .txt file in your specified directory. sink must sandwich your code that you want to save.\n\nFor example: \n\nsink(\"output/console_content.txt\")\n\nRun code of your choice\n\nsink()\n\nA2: If using RStudio, you can do this via point and click. Navigate to \"History\" in the top right window. Click \"Save\". You must do this before ending the session. The \"Save\" option that comes up upon closing the session will save a hidden . Rdata file, not your console output as a separate file. \nQ2: How does ‘filter()’ work?\nA: The 'filter()' function from the 'dplyr' package is used to subset data frames based on specified conditions. Specify the data frame and condition as follows:\n\nfilter(.data, condition)\n\nHere .data is any data frame in your environment that you want to filter. Condition needs to be specified to filter the data frame however you need. filter() will search through each column (or a specified column) until it finds a row that meets the specified condition. Any row that has a value that meets your condition will be removed. Typically when we filter we assign the filtered data frame to a new name, or overwrite the original data frame. For example:\n\nfiltered.data &lt;- filter(original.data, original.data$frequency &gt; 0.05)\n\nfiltered.data have rows with frequency &gt; 0.05 removed. \nQ3: What is the difference between a function and an operator?\nA: A function is a chunk of code that is designed to perform a specific task. They typically wrap multiple arguments and use complex logic that is inefficient to rewrite each time it is needed. A function streamlines the process so all you have to do is call the function, specify a small number of options. This is typically done in one line of code. \n\nAlternatively, an operator is a simple symbol that is used to perform arithmetic, logical, or rational operations on data. For example a '+' is an arithmetic operator for addition. \nQ4: Why does the ‘is.na()’ function work if the NA in my data frame is uppercase? Isn’t it case-specific?\nA: Though most things in R are case specific, is.na() isn't actually looking for the specific \"NA\" text. NA comes up as an empty or missing value. Therefore, even though we see NA as uppercase, in R it comes up as a missing value, so the case of the text doesn't matter. Where is.na() finds a missing value, it will return a value of 'TRUE'. Actual values will be returned as 'FALSE'",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>MaRcus R Training</span>"
    ]
  },
  {
    "objectID": "dataexport.html",
    "href": "dataexport.html",
    "title": "6  Exporting and Importing Data Formats in R",
    "section": "",
    "text": "6.1 Description\nThis script will demonstrate methods for exporting and importing various data and plot formats from an R script. We will be using the built-in “iris” and “mtcars” datasets available in R. We encourage you to go through these steps with a dataset of your own and export formats that are relevant to your study. This session will cover commonly needed formats, including .xlsx, .csv, .pdf, .png, and .jpeg. However, there are many additional data formats that can be used and we recommend exploring these independently. Keep in mind that there are many different ways to do similar things in R, i.e. multiple packages to export to .xlsx. This script is intended to provide some helpful examples, but is not comprehensive.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exporting and Importing Data Formats in R</span>"
    ]
  },
  {
    "objectID": "dataexport.html#description",
    "href": "dataexport.html#description",
    "title": "6  Exporting and Importing Data Formats in R",
    "section": "",
    "text": "6.1.1 Clear environment\n\nls()\nrm(list=ls())\n\n\n\n6.1.2 Set output directory\n\ndir.create(\"output\")\ndir_save &lt;- \"output/\"\n\n\n\n6.1.3 Load libraries\n\nlibrary(tidyverse) # Needed for 'glimpse()'\nlibrary(openxlsx) # Needed to export data.frame to .xlsx\nlibrary(dplyr) # Needed to convert rownames to column and simultaneously delete rownames\nlibrary(rio) # Needed for 'import' function \nlibrary(readxl) # Needed for alternative method for importing .xlsx\n\n\n\n6.1.4 Load datasets\nWe will load the built-in “iris” and “mtcars” datasets for demonstration purposes.\n\ndata(\"iris\")\ndata(\"mtcars\")\n\n\n\n6.1.5 Examine data structure\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\nclass(iris)\n\n[1] \"data.frame\"\n\nclass(mtcars)\n\n[1] \"data.frame\"\n\n\n\n\n6.1.6 Export data to .xlsx\nHere we will use dir_save to specify where we want to save our files. Alternatively, you can write out the full path to your output directory.\n\n# To export a single data.frame to .xlsx \n\nwrite.xlsx(iris, paste0(dir_save, \"iris_data.xlsx\"))\n\n\n# To export multiple data.frames into different sheets, create a list of data.frames to be used as the object for write.xlsx\n\ndata.frames &lt;- list('Sheet1' = iris, 'Sheet2' = mtcars)\nwrite.xlsx(data.frames, file = paste0(dir_save, \"iris_mtcars_data.xlsx\"))\n\n\n# Write to .xlsx including colnames and rownames for all sheets\n\nwrite.xlsx(data.frames, file = paste0(dir_save, \"iris_mtcars_data_colrow.xlsx\"), colNames = TRUE, rowNames = TRUE)\n\n\n# Alternatively, convert rownames from specific data.frames to a named column and export without rownames\n\nmtcars &lt;- tibble::rownames_to_column(mtcars, \"Model\")\ndata.frames &lt;- list('Sheet1' = iris, 'Sheet2' = mtcars)\nwrite.xlsx(data.frames, file = paste0(dir_save, \"iris_mtcars_data_rownamestocol.xlsx\"))\n\n\n\n6.1.7 Export data to .csv\n\n# Let's first export iris as is and restore mtcars to its original format before exporting to .csv\n\nwrite.csv(iris, file = paste0(dir_save, \"iris_data.csv\"))\n\nmtcars &lt;- column_to_rownames(mtcars, var = \"Model\")\nwrite.csv(mtcars, file = paste0(dir_save, \"mtcars_data.csv\"))\n\n# You'll notice that the default for write.csv is to set col.names and row.names = TRUE\n\nwrite.csv(mtcars, file = paste0(dir_save, \"mtcars_data_colrowfalse.csv\"), col.names = FALSE, row.names = FALSE)\n\nWarning in write.csv(mtcars, file = paste0(dir_save,\n\"mtcars_data_colrowfalse.csv\"), : attempt to set 'col.names' ignored\n\n# When using write.csv, colnames will still be written. If you want to eliminate colnames, use write.table\n\nwrite.table(mtcars, file = paste0(dir_save, \"mtcars_data_colfalse.csv\"), col.names = FALSE, row.names = FALSE)\n\n\n\n6.1.8 Import data from .xlsx\n\n# Import a data.frame from a specific sheet in a .xlsx file\n\ndf.iris.xlsx &lt;- read.xlsx(xlsxFile = \"output/iris_mtcars_data_colrow.xlsx\",\n                     sheet = 1,\n                     rowNames = TRUE)\n\nclass(df.iris.xlsx)\n\n[1] \"data.frame\"\n\nhead(df.iris.xlsx)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# A common alternative method relies on the 'readxl' package, but functions differently\n\ndf.mtcars.xlsx &lt;- read_xlsx(\"output/iris_mtcars_data_colrow.xlsx\",\n                    sheet = 2)\n\nNew names:\n• `` -&gt; `...1`\n\nclass(df.mtcars.xlsx)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nhead(df.mtcars.xlsx)\n\n# A tibble: 6 × 12\n  ...1           mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n6 Valiant       18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n\n# Using this method, you will need to convert to a data.frame before you can set rownames\n\ndf.mtcars.xlsx &lt;- as.data.frame(df.mtcars.xlsx)\nrownames(df.mtcars.xlsx) &lt;- df.mtcars.xlsx[[1]]\ndf.mtcars.xlsx &lt;- df.mtcars.xlsx[-1]\nhead(df.mtcars.xlsx)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n6.1.9 Import data from .csv\n\n# Import the iris data.frame as is. Below are two alternative methods.\n\ndf.iris.csv &lt;- read.csv(\"output/iris_data.csv\")\n\ndf.iris.csv &lt;- import(\"output/iris_data.csv\")\n\n# Import and set colnames\n\ndf.iris.csv &lt;- read.table(\"output/iris_data.csv\", row.names = 1, header = TRUE, sep = \",\")\n\nhead(df.iris.csv)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ndf.mtcars.csv &lt;- read.table(\"output/mtcars_data.csv\", row.names = 1, header = TRUE, sep = \",\")\n\nhead(df.mtcars.csv)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n6.1.10 Plot data and export\n\n# Create a plot and save using ggplot followed by ggsave\n  \nggplot(data = df.iris.csv, \n                mapping = aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point(aes(color = Petal.Width)) +\n  geom_smooth(method=\"lm\") +\n  labs(title = \"Petal vs. Sepal Length\", subtitle = \"Separated by Species\", x = \"Sepal Length\", y = \"Petal Length\") + \n  facet_wrap(~Species, \n             scales = \"free_x\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\nggsave(\"output/iris_ggplot.pdf\", width = 7, height = 7)\nggsave(\"output/iris_ggplot.png\", width = 7, height = 7)\nggsave(\"output/iris_ggplot.jpeg\", width = 7, height = 7)\n\n# Alternatively, assign the plot to an object, then print and dev.off. Whereas the first method is compatible with plots made using ggplot, this method will work for any type of plot. \n\nplot &lt;- ggplot(data = df.iris.csv, \n                mapping = aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point(aes(color = Petal.Width)) +\n  geom_smooth(method=\"lm\") +\n  labs(title = \"Petal vs. Sepal Length\", subtitle = \"Separated by Species\", x = \"Sepal Length\", y = \"Petal Length\") + \n  facet_wrap(~Species, \n             scales = \"free_x\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\npdf(\"output/iris_plot.pdf\", width = 7, height = 7)\nprint(plot)\ninvisible(capture.output(dev.off()))\n\npng(filename = \"output/iris_plot.png\", width = 1500, height = 1500, res = 300)\nprint(plot)\ninvisible(capture.output(dev.off()))\n\njpeg(\"output/iris_plot.jpeg\", width = 1500, height = 1500, res = 300)\nprint(plot)\ninvisible(capture.output(dev.off()))\n\n\n\n6.1.11 Save what has been done to an .Rdata file\nIn some cases, it may be helpful to save a specific object or everything in your environment to an .Rdata file that can be imported all at once to be used in a different pipeline or at a later time. You can save as either an RData object or as an RDS object.\n\n# To save a specific object\n\nsave(df.iris.csv, file = paste0(dir_save, \"df.iris.csv.RData\"))\n\n# To save all data and values in your R environment to an RData file\n\nsave.image(paste0(dir_save, \"Data_Export_Tutorial.RData\"))\n\nYou can then load that .RData file back into R and start back up where you left off.\n\n# First clear the environment so we can see how RData files are loaded\n\nls()\nrm(list=ls())\n\n# Now load your .RData objects\n\nload(\"output/Data_Export_Tutorial.RData\")\n\nYou can do the same thing for single objects saved as .RDS\n\nsaveRDS(df.iris.csv, file = paste0(dir_save, \"df.iris.csv.rds\"))\n\nls()\nrm(list=ls())\n\n# Now load your .RDS objects\n\nreloaded_data &lt;- readRDS(\"output/df.iris.csv.rds\")\n\nThere is a workaround to save and reload an entire environment as .RDS, but it is a bit more involved and requires the use of loops, which is beyond the scope of this session. We will cover loops in a later session.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exporting and Importing Data Formats in R</span>"
    ]
  },
  {
    "objectID": "dataexport.html#homework",
    "href": "dataexport.html#homework",
    "title": "6  Exporting and Importing Data Formats in R",
    "section": "6.2 Homework",
    "text": "6.2 Homework\nFor this homework assignment, you will be using a script that you write yourself! If you have data for your own study, we suggest writing a simple script that is relevant to the analyses you will need to do. The only requirements are that you should use data that can be imported / exported in a table or dataframe format and plotted. If you do not have data of your own yet, you can use a built in dataset available from R. To find built in datasets use the following command:\ndata()\nNow perform the following steps:\n\nClear your environment.\nSet your working directory. This should be in a location where you perform work related to this course.\nSet output directory. This should be a subdirectory within your working directory where you want to save any files that you generate. You can create this manually in your normal file finder or create it using R as is done in the script above.\nLoad libraries that are necessary for your script.\nLoad your dataset. Either import your own data or load one of the built in datasets.\nExamine data structure.\nPlot your data however you like! Refer to previous sessions for ideas and guidance.\nSave your plots as pdf, png, and jpeg.\nExport your data file as .xlsx and .csv. Confirm that your row and colnames are in the correct position.\nSave a relevant object from your environment as .Rdata and .rds.\nLoad your .Rdata and .rds files back into R.\nConsult the internet or ChatGPT and find at least one alternative method to import, export, and save your data or plots. Try these out.\nSave your script.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exporting and Importing Data Formats in R</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "7  Introduction to Git and GitHub",
    "section": "",
    "text": "7.1 Description\nThis session will cover the basics of using Git and GitHub to create version-controlled analyses and projects. Before this session you should have set up at GitHub account and installed GitHub Desktop. For instructions, visit the “Installations” session document. It is also recommended that you install the GitHub CLI to facilitate streamlined interfacing between github and the Terminal.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#description",
    "href": "github.html#description",
    "title": "7  Introduction to Git and GitHub",
    "section": "",
    "text": "7.1.1 What is Git?\nGit is the most widely used version control system to date. It is free and open source. Git-based version controlling allows users to track changes that have been made to documents, code, etc. It also gives users the ability to restore version-controlled documents to earlier versions and collaborate with other developers.\nSome common vocabulary:\n1. Directory = Folder\n2. Repository = Parent folder which is the top folder for your project\n3. Commit = a \"saved\" snapshot of the repository or files within it \n4. Push = upload the current version of your repository to GitHub\n5. Pull = download content from a remote GitHub repository and update the local repository to the pulled version \n6. Clone = copy a remote GitHub repository to a local location \n7. Staging area = contains information about what you will include in your next commit\n8. Terminal = a.k.a. command line, interface for text commands\n9. CLI = command line interface, allows certain programs to interface with the command line (for example, quarto has a CLI that allows you to run quarto commands from the terminal in RStudio)\nWhat is the difference between Git, GitHub, and GitHub Desktop?\nGit is the tool that actually tracks changes made to your code over time and allows for version control. GitHub is an online website that stores your Git repositories. GitHub Desktop is a downloaded software that allows you to work with your Git repositories locally.\n\n\n7.1.2 Basic git commands:\n1. git clone = copy a GitHub repository to your local machine\n2. git add = add files to the git staging area\n3. git commit = save files (typically with a message describing what was changed)\n4. git push = upload files in the commit to a remote repository (GitHub)\n5. git pull = download remote repository to local directory and update local repository with remote version \n\n\n7.1.3 Interface with git from your local machine using the Terminal\nMost of the time, we use git on our local machine to develop the contents of the repo and then push them to GitHub. For this session, we will be interfacing with git from the Terminal to build an understanding of how git operates on a fundamental level. In our next session, we will cover how to develop git-controlled projects using the code editor VS Code.\n1. Confirm git is installed. Open Terminal and type the following:\nNote - If you can’t find your Terminal, (for Mac) you can press Cmd + space to open Spotlight search. Type “Terminal”. We recommend adding Terminal to your dock.\n\ngit --version\n\nYou should see a readout that looks like this:\n\n2. Configure git\n\nThis will allow you to set your user information so it will stay consistent across all git commits and repositories. You will only need to do this once. \n\n# To set the name attached to your commits\ngit config --global user.name \"Your Name\"\n\n# To set the email attached to your commits\ngit config --global user.email \"youremail@email.com\"\n\n3. Navigate to your local working directory where you would like to store your git repository.\n\ncd /PATH/TO/WORKING/DIRECTORY\n\n4. Make a directory for your project and initialize git. \n\nmkdir [PROJECT NAME]\n\ncd [PROJECT NAME]\n\ngit init\n\n5. Create your files locally. \n\n      - You can populate the repo with any files that you like. Be it a Quarto project, a simple R script, or a data folder. \n\n      - You should always have .gitignore and README.md files in your repository. .gitignore should be added before anything else because if you begin adding and committing files before you specify which should be ignored, they will already be tracked. \n      \nNote - You do not need to do this manually if creating a Quarto Project as these files are already included\nNote - the .gitignore file will be automatically hidden in your file finder. If you want to find and edit your .gitignore, on a Mac, you can use the keyboard shortcut Cmd + shift + . to view it or open the repo in a code editor (this will be covered later).\n\n# To create a .gitignore from the Terminal\n\ntouch .gitignore\n\n\n# To create a README.md from the Terminal\n\ntouch README.md\n\nIt is generally good practice to now set up a basic repo structure. First edit the .gitignore to ignore any large folders or files that you do not want to track (for large analyses, consider ignorming data and output folders)\nMake a directory for data. This should generally contain raw unprocessed data.\n\nmkdir data\n\nMake a directory for output. This should generally contain any of the processed outputs (processed data, figs, etc) generated by your code.\n\nmkdir output\n\n6. Check the status of your repo. \n\ngit status\n\n7. Add your new or edited files to the staging area. \n\n      - The git staging area is an intermediate platform between working files and permanently stored version-controlled commits. This allows you to continue working with any files and confirm they are correct before committing. Any files added to the staging area will be committed in the next commit. \n\n# To add one file \ngit add file.txt\n\n# To add multiple files\ngit add file.txt file2.csv\n\n# To add files by pattern\ngit add *.text\n\n# To add all files in the directory recursively \ngit add .\n\nNote - Be careful when adding all files recursively, because it will also add large files if they are not specified in .gitignore\nCheck status again to confirm correct files have been added to the staging area.\n\ngit status\n\n8. View unstaged changes. \n\n      - It is good practice to view the changes that have been made before committing. Normally we do this in a visual code editor (i.e. VS Code), but for now we will go over how to do this from the Terminal. \n\n# To see staged changes\ngit diff --staged\n\n# To exit, press \"q\"\n\n#To see unstaged changes\ngit diff\n\n9. If necessary, unstage files. \n\n      - If you view your changes and notice an error (i.e. you have staged a large file that you don't want to track), you can remove it from the staging area or \"unstage\" it before you commit. \n\n# To unstage a specific file\ngit restore --staged file.txt\n\n# or\ngit rm --cached file.txt\n\n# To unstage multiple files \ngit restore -- staged file.txt file2.csv\n\n# To unstage files by pattern\ngit restore --staged *.txt\n\n# To unstage everything in the staging area while maintinging changes the files\ngit reset .\n\n10. Commit changes\n\n      - Once you are happy with the files in your staging area you can commit to save the changes and the current snapshot of the repository. \n\n*Note - It is good practice to always include a short \"message\" with each commit that describes what was changed. This should be written in present tense. Your first commit is generally called \"Initial commit.\"*\n\n# The -m option will include the message for your commit\n\ngit commit -m \"Initial commit.\"\n\n11. Push repo to GitHub.\nTo update the online remote repository, you need to push the repo to GitHub. An easy way to do this is to use the GitHub CLI. If you have not done so already, download and install.\n\n# To push using GitHub CLI, authenticate GitHub\ngh auth login\n\n# Follow the prompts in the Terminal\n\n# Create a remote repository\ngh repo create\n\n# To push the repository we created here, you must select \"Push an existing local repository to GitHub\". You can also use gh repo create to create a new repository from scratch or from a template. To push the existing local repo, paste the file path at the prompt\n\n?Path to local repository (.) /PATH/TO/LOCAL/REPO\n\nNote - If you already have a remote repository created that is cloned to a local directory, after making changes locally, you can push the local repository to the remote repository using the following:\n\ngit push\n\n\n\n7.1.4 Branching and Merging from the Terminal\nOne main feature of git repositories is the ability to create and merge different branches. This comes in handy when you have multiple people working on the same project. You can work on different branches that are dedicated to different parts of the project, and then those branches can be merged back into the master branch.\n1. Create and switch to a branch\n\n# To create a new branch called \"dev\"\ngit branch dev\n\n# To begin working in the \"dev\" branch\ngit checkout dev\n\n# Alternatively, do do this in one step\ngit checkout -b dev\n\nNow you can make your changes and commit on that branch.\n2. Merge the branch back into the master branch. \n\ngit merge dev\n\n3. Now return to the main branch\n\ngit checkout master\n\n# Or\ngit checkout main\n\n\n\n7.1.5 Using git commands to navigate through git architecture from the Terminal\nThis diagram from unstop may be helpful for understanding how git commands can be used to move files around between local and remote directories. The webpage also gives a thorough description of how git and GitHub work.\n\n\n\nhttps://unstop.com/blog/what-is-git\n\n\n\n\n7.1.6 Create, branch, and clone a repository from GitHub\nThink of a git repository as a project folder. All of the documents that you will need to run analyses for your project should be stored within the same repo.\n1. Sign in to [GitHub](https://github.com/login).\n\n2. From your homepage, click the \"New\" button to create a new repository.\n\n3. Give the repo a name and description and adjust settings if necessary. \n\n          - We typically set the repo to private until it is ready to share publicly. \n\n          - You can add README.md and .gitignore files here if you like, or you can add them later. If you are creating a project locally, it often makes more sense to create these files locally. For some projects, for example, a Quarto Project, these files are already included in the Quarto Project architecture, so you do not need to create them here. \n\n4. If necessary, add README.md on GitHub. \n\n        - All projects MUST have a README file. This is a Markdown file that should contain all relevant information to describe the repository and instructions for running any analyses. \n\n\n5. Create a new branch. \n\n6. Clone your remote repo to your working directory. \n\n      - There are a number of ways to do this. To clone the repo from the command line, go to the online repo on GitHub, click \"Code\" and select your method. Copy the path and replace it into the line of code shown below in the Terminal: \n\ngit clone https://github.com/SATVILab/SATVI_ComputationalCourse.git\n\n\n\n7.1.7 Create, branch, and push a repository from GitHub Desktop\n1. Open GitHub Desktop and sign in.\n\n2. Add a repository. This repo can either be cloned from GitHub, created as a new repo, or added from an existing local repo. \n\n3. Branch the repo. \n\n4. Push the repo to GitHub.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#cheatsheets",
    "href": "github.html#cheatsheets",
    "title": "7  Introduction to Git and GitHub",
    "section": "7.2 Cheatsheets",
    "text": "7.2 Cheatsheets\nHere are two helpful git sheat sheets:\nGit Cheat Sheet\nGitLab Cheat Sheet",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#homework",
    "href": "github.html#homework",
    "title": "7  Introduction to Git and GitHub",
    "section": "7.3 Homework",
    "text": "7.3 Homework\n1. Read the blog post \"What is GIT\" from unstop\nunstop “What is GIT”\n2. Set up 2-Factor Authentication on your GitHub account. This will be required to access the SATVILab GitHub organization. Contact Miguel Rodo (rdxmig002@myuct.ac.za) to be added to the SATVILab organization. \nGitHub 2-Factor Authentication\n3. Create a git repository from the Terminal and push to GitHub. This can be a test repo or, ideally, a repo that contains files related to your own project. \n\n4. Download and install VS Code.\n\n5. If it interests you, download and framework for git-aware terminal configuration, such as oh my zsh. Configure the settings to your liking. \noh my zsh",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Git and GitHub</span>"
    ]
  },
  {
    "objectID": "vscode.html",
    "href": "vscode.html",
    "title": "8  Introduction to Visual Studio Code",
    "section": "",
    "text": "8.1 Description\nThis session will cover an introduction to using Visual Studio Code (VS Code). VS Code has many features and functions that we will not have time to cover in this introductory session. It is very versatile and used by developers and computational biologists alike. We encourage you to explore in your own time and consider using VS Code as an additional tool in you computational kit.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Visual Studio Code</span>"
    ]
  },
  {
    "objectID": "vscode.html#description",
    "href": "vscode.html#description",
    "title": "8  Introduction to Visual Studio Code",
    "section": "",
    "text": "8.1.1 What is VS Code?\nVS Code is a commonly used code editor that incorporates many of the same features as RStudio, but allows for additional functionalities such as debugging, extensions, and version control interfacing.\nWhat we love about VS Code - one place for everything!\nVS Code allows you to create new projects and files, switch between coding languages, create, edit, debug, push, pull, and version control code all from one place.\nSome common vocabulary:\n\nCode editor = a text editor program that is designed for editing source code and can identify code elements and errors in real time.\n\n\n\n8.1.2 Initialize a project in VS Code\n\nOpen the VS Code desktop application. You should see the VS Code “Welcome” page.\n\nFrom the Welcome page, you can create a new file, open an existing or recent project or file, clone a git repository, or connect to a remote development workspace. These options can also be found under the “File” tab.\nThe Welcome page also contains liks to helpful “Walkthroughs” which provide tutorials for tasks and functionalities you might find useful while you develop your code.\nToday we will create a new Quarto project, add files, and initialize git, and push to GitHub.\n\n\n\n\nConsider checking “Auto Save”\n\n\n\nCreate a new Quarto project\n\nWhen you click “New File”, a pop-up will open where you can select the type of file you want to generate. We will select “Quarto Project”\n\n\n\n\nNext select the type of project you will want to create. We will create a Quarto Book.\n\n\n\nSelect the directory where you want to save your project and give it a name.\n\n\n\nYou will see your Quarto Book project directory and automatically generated base files appear in the left panel in VS Code.\n\n\nThe panels in VS Code are customizable. Some key features are:\nUser interface\nThe VS Code user interface contains all of the necessary components to develop your code. Each component is described below.\n Activity Bar\nControls the view of the Primary Side Bar and houses extensions.\nPrimary Side Bar\nWhat you see here will depend on what you have selected from the Activity Bar. We often have this set to “Explorer” which shows you the files within your directory and project. The Explorer pane can also show your outline, timeline, containers, etc that are relevant for the open project. These are all collapsable.\nYou can also use the Activity Bar to open a Search tool, Source Control, Debugging tool, Extensions, Remote Explorer (i.e. GitHub Codespaces), or Containers in the Primary Side Bar.\nEditor\nThis is where you do your work. Like the scripts panel in RStudio, this is where you will open and edit files.\n- Multiple panels - Change the Editor Layout from the \"View\" tab or from the four icons on the top right of the VS Code window to view multiple files simultaneously. Editors can be split vertically or horizontally. \n      \n\n\nPanel\nThis is a versatile additional space where you can run code from the command line, view debugging information, background jobs etc. Importantly, this is where you can access your Terminal directly within VS Code. We will discuss this in further detail below.\nStatus Bar\nThe Status Bar provides information about the open files and project, status of git-controlled repositories, and information on if scripts are currently being run.\n\n\n8.1.3 Install Extensions\nWhen you first install VS Code, you should have no extensions. As different code will require different extensions, you will customize you list of installed extensions as you develop. You can explore available extensions directly from the VS Code Extensions pane, or you can browse the VS Code Marketplace. If you start a script that requires a certain extension, VS Code will prompt you to install it. However, there are some basic extensions that most users will need.\nTo install an extension, simply click on the Extensions icon from the Activity Bar and search for your needed extension or choose from the list of recommended extensions. When you click on a desired extension it will display associated documentation in your Editor. Click “Install” and check that it appears in your “Installed” section in the Extensions pane.\nSome recommended extensions are below. We have provided links to the documentation on VS Code Marketplace so you can read about these while you are in the process of setting up VS Code on your machine, but we recommend that eventually you install these directly in VS Code desktop, rather than from the links provided here:\nCode Runner\nR\nvscodeR\nQuarto\nGitHub Repositories\nThere are also some extensions dedicated to debugging which you may find useful, such as R Debugger.\nUse the Extensions pane to manage and uninstall extensions as needed.\n\n\n8.1.4 Manage files\nTo manage the files in your directory, you do not have to exit VS Code and manage files from your standard file finder. The Explorer pane in your Primary Side Bar can serve as a file finder and manager.\n\nTo rename a file in Explorer:\n\nRight click the file and rename\n\nTo delete a file from Explorer:\n\nRight click the file and delete\n\nTo change the file type in Explorer:\n\nRight click and change the file extension. For example if you create a new Python file called newfile.py, but actually wanted it to be JavaScript, you can just change the file extension to .js.\n\n\n\nNotice that if you try to create a new file that needs a certain extension to run, VS Code will prompt you to install the recommended extensions.\n ## Initialize Git\nOne major benefit of using VS Code is that it enables integration of your code editor and terminal, streamlining the process for initializing and managing git-controlled projects.\nLet’s use what we learned from the last session to initialize git for our current directory from VS Code.\n\nOpen a new Terminal in your VS Code panel.\n\n\n\nFollow the steps from our session on git and GitHub to initialize git on the project repository, add all files to the staging area, commit, and push to GitHub.\n\n\n# Initialize git\ngit init\n\n# Create .gitignore\ntouch .gitignore\n\n# Create README.md\ntouch README.md\n\n# Check git status\ngit status\n\n# Add all files in the directory to staging area recursively\ngit add .\n\n# Check git status\ngit status\n\n# Make initial commit\ngit commit -m \"Initial commit.\"\n\n# Authenticate GitHub\ngh auth login \n\n# Create a remote repository \ngh repo create\n\n# Set path to existing local repository when prompted\n?Path to local repository (.) /PATH/TO/LOCAL/REPO\n\n# Push existing local repository \ngit push",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Visual Studio Code</span>"
    ]
  },
  {
    "objectID": "vscode.html#code",
    "href": "vscode.html#code",
    "title": "8  Introduction to Visual Studio Code",
    "section": "8.2 Code",
    "text": "8.2 Code\nYou can now add and edit your files however you like! Add and edit files, just remember to continue to commit and push as you go. Let’s create a simple .qmd file with the code below and save it as a new file to our repository:\n\n8.2.1 Clear environment\n\nls()\nrm(list=ls())\n\n\n\n8.2.2 Set output directory\n\ndir.create(\"output\")\ndir_save &lt;- \"output/\"\n\n\n\n8.2.3 Load libraries\n\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\n\n8.2.4 Load dataset\nWe will load the built-in “iris” R dataset and examine structure.\n\ndata(\"iris\")\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n\n8.2.5 Plot data and export\n\nggplot(data = iris, \n                mapping = aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point(aes(color = Petal.Width)) +\n  geom_smooth(method=\"lm\") +\n  labs(title = \"Petal vs. Sepal Length\", subtitle = \"Separated by Species\", x = \"Sepal Length\", y = \"Petal Length\") + \n  facet_wrap(~Species, \n             scales = \"free_x\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\nggsave(\"output/iris_ggplot.pdf\", width = 7, height = 7)\nggsave(\"output/iris_ggplot.png\", width = 7, height = 7)\nggsave(\"output/iris_ggplot.jpeg\", width = 7, height = 7)\n\nRender the .qmd file.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Visual Studio Code</span>"
    ]
  },
  {
    "objectID": "vscode.html#debugging",
    "href": "vscode.html#debugging",
    "title": "8  Introduction to Visual Studio Code",
    "section": "8.3 Debugging",
    "text": "8.3 Debugging\nOne of the key features VS Code supports is debugging. VS Code has a built in debugger that is compatible with JavaScript and TypeScript, but for other languages, you, like R, you will need to install an extension to help debug the code. For R, the debugging extension is R Debugger.\nFollow the instructions in the README.md to install and deploy R Debugger.\nBriefly:\n\nInstall R Debugger extension in VS Code.\nInstall R package vsDebugger().\n\n\ndevtools::install_github(\"ManuelHentschel/vscDebugger\")\n\n\nOpen you R file in the VS Code editor pane.\nClick the debugger from the Activity Panel so it opens in the Primary Sidebar.\nClick F5 or the “Launch” button in the Debugger Activity Sidebar.\n\nYou may also want to open the DEBUG CONSOLE in your panel.\n\nClick F5 or “Continue” to debug your code.\n\nLet’s try with a very simple example. Create a simple R script in your open directory:\n\nprint(\"Hello World\")\n\nRun the code to make sure it works. Then introduce an intentional error:\n\nprnt(\"Hello World\")\n\nNow launch R Debugger and view the errors. You should see that R Debugger has identified the issue so you can fix it:\n\nVS Code also supports extensions for AI assistants that can help debug code as you work, such as GitHub Copilot:\nGitHub Copilot\nGitHub Copilot allows you to interface with a ChatGPT-like AI tool to help with your code as you develop. It is a paid extension, but there is a free trial.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Visual Studio Code</span>"
    ]
  },
  {
    "objectID": "vscode.html#cheatsheets",
    "href": "vscode.html#cheatsheets",
    "title": "8  Introduction to Visual Studio Code",
    "section": "8.4 Cheatsheets",
    "text": "8.4 Cheatsheets\nVS Code Keyboard Shortcuts - macOS\nVS Code Keyboard Shortcuts - Windows",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Visual Studio Code</span>"
    ]
  },
  {
    "objectID": "vscode.html#homework",
    "href": "vscode.html#homework",
    "title": "8  Introduction to Visual Studio Code",
    "section": "8.5 Homework",
    "text": "8.5 Homework\n\nDownload and install VS Code.\nPlay around VS Code.\nEdit your git-controlled repository, commit, and push to GitHub, all within VS Code.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Visual Studio Code</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html",
    "href": "hypothesistesting.html",
    "title": "9  Hypothesis testing",
    "section": "",
    "text": "9.1 Why bother with statistics?\nSuppose we have a spreadsheet with two variables:\nCode\nset.seed(123)\nx_vec &lt;- runif(100, 0, 1)\ny_vec &lt;- x_vec + rnorm(100, 0, 0.1)\ndata_tbl_bother &lt;- tibble::tibble(\n  x = x_vec,\n  y = y_vec\n)\nCode\np_bother &lt;- ggplot(data_tbl_bother, aes(x = x, y = y)) +\n  geom_point(colour = \"dodgerblue\", alpha = 0.8) +\n  theme_cowplot_custom() +\n  labs(x = \"Variable 1\", y = \"Variable 2\")\n# save plot\npath_p &lt;- file.path(path_dir_fig, \"p-why-bother-init.png\")\nggsave(path_p, p_bother, width = 10, height = 10, units = \"cm\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#why-bother-with-statistics",
    "href": "hypothesistesting.html#why-bother-with-statistics",
    "title": "9  Hypothesis testing",
    "section": "",
    "text": "9.1.1 Performing inference\nToday, we’ll talk about choosing appropriate statistical approaches to detect effects in our data.\nIn particular, we’ll mention useful R functions for performing these tasks, such as the following:\n\ncorr_test_obj &lt;- cor.test(data_tbl_bother$x, data_tbl_bother$y)\ncorr_test_obj\n\n\n    Pearson's product-moment correlation\n\ndata:  data_tbl_bother$x and data_tbl_bother$y\nt = 28.992, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9211609 0.9636474\nsample estimates:\n      cor \n0.9463529 \n\n\nWe will also discuss how to extract the key information (estimate, p-value, etc.) and present it in a more readable format:\n\n\n\nCorrelation\nConfidence interval\nP-value\n\n\n\n\n0.946\n0.921 to 0.964\n0",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#understanding-terms",
    "href": "hypothesistesting.html#understanding-terms",
    "title": "9  Hypothesis testing",
    "section": "9.2 Understanding terms",
    "text": "9.2 Understanding terms\n\n9.2.1 Hypothesis testing\n\nPurpose: detect differences\nExamples:\n\nIs the correlation between two variables different from zero?\nIs the effect of a drug different from a placebo?\n\nPrimary tool for accounting for uncertainty: P-value\n\n\n\n9.2.2 Estimation\n\nPurpose: obtain best estimate for a given value from the data\nExamples:\n\nWhat is the correlation between two variables?\nWhat is the effect of a drug?\n\nPrimary tool for accounting for uncertainty: confidence intervals\n\n\n\nDetails",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#relationship-to-the-data",
    "href": "hypothesistesting.html#relationship-to-the-data",
    "title": "9  Hypothesis testing",
    "section": "9.3 Relationship to the data",
    "text": "9.3 Relationship to the data\n\nP-values measure the compatibility of the data with the null hypothesis\nConfidence intervals provide a range of values that will contain the true value with a certain probability\n\nFor example, a 95% confidence interval [0.04, 0.5] says that 95% of the time, the true value will fall within the interval [0.04, 0.5]\n\n\n\n9.3.0.1 Hypothesis testing\n\nNull hypothesis (\\(H_0\\)): The default assumption\n\nTypically, that there is no effect/difference\nFor example:\n\nThe correlation between two variables is zero\nThe effect of a drug is no different from a placebo\n\n\nThe null hypothesis is tested against the alternative hypothesis (\\(H_1\\))\n\nFor example:\n\nThe correlation between two variables is not zero\nThe effect of a drug is different from a placebo\n\n\nP-value:\n\nThe probability of observing a value at least as extreme as what we did observe, given that the null hypothesis is true\n\n\n\n:::",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#the-primary-challenge",
    "href": "hypothesistesting.html#the-primary-challenge",
    "title": "9  Hypothesis testing",
    "section": "9.4 The primary challenge",
    "text": "9.4 The primary challenge\nThe main difficulty in performing inference well lies in choosing the appropriate method for the task at hand. Inappropriate choices can be disastrous. Here, for example, two different correlation coefficients will give very different results, because one’s assumptions make it vulnerable to outliers:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#side-skipping-the-difficulties",
    "href": "hypothesistesting.html#side-skipping-the-difficulties",
    "title": "9  Hypothesis testing",
    "section": "9.5 Side-skipping the difficulties",
    "text": "9.5 Side-skipping the difficulties\nThe easiest way to avoid making erroneous assumptions is to not make any. This is the reason for the undying (and well-deserved) popularity of non-parametric methods, which make no assumptions about the underlying distribution of the data. Chief among them are:\n\nCorrelation:\n\nThe Spearman rank correlation\n\nHypothesis testing:\n\nThe Wilcoxon rank-sum test (Mann-Whitney/Mann-Whitney U/Wilcoxon-Mann-Whitney/…)\nThe Kruskal-Wallis test\n\nConfidence intervals:\n\nThe bootstrap method\n\n\nFor this section, we will focus on hypothesis testing.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#spearman-rank-correlation",
    "href": "hypothesistesting.html#spearman-rank-correlation",
    "title": "9  Hypothesis testing",
    "section": "9.6 Spearman rank correlation",
    "text": "9.6 Spearman rank correlation\nThe Spearman rank correlation test is a non-parametric test that assesses the strength and direction of association between two ranked variables.\nIt is robust against outliers because it uses *ranks$ instead of the actual values of the variables.\n\n9.6.1 Ranks\nHere is what ranks look like, as a table:\n\nCode\nrank_tbl &lt;- tibble(\n  Value = data_tbl_error$x[seq_len(5)] |&gt; signif(2)\n) |&gt;\n  mutate(\n    Rank = rank(Value)\n  )\nrank_tbl |&gt; knitr::kable()\n\n\n\n\nValue\nRank\n\n\n\n\n0.29\n1\n\n\n0.79\n3\n\n\n0.41\n2\n\n\n0.88\n4\n\n\n0.94\n5\n\n\n\nHere’s what the look like, plotted:\n\n\nCode\np_error &lt;- ggplot(data_tbl_error, aes(x = x, y = y)) +\n  geom_point(colour = \"dodgerblue\", alpha = 0.8) +\n  theme_cowplot_custom() +\n  labs(x = \"Variable 1\", y = \"Variable 2\")\np_error_ranked &lt;- ggplot(\n  data_tbl_error |&gt;\n    dplyr::mutate(x = rank(x), y = rank(y)),\n  aes(x = x, y = y)) +\n  geom_point(colour = \"dodgerblue\", alpha = 0.8) +\n  theme_cowplot_custom() +\n  labs(x = \"Variable 1 (ranks)\", y = \"Variable 2 (ranks)\")\np_grid_error_rank &lt;- plot_grid(\n  p_error + labs(title = \"Original\"),\n  p_error_ranked + labs(title = \"Ranked\"),\n  nrow = 1\n)\n# save plot\npath_p &lt;- file.path(path_dir_fig, \"p-error-rank.png\")\nggsave(path_p, p_grid_error_rank, width = 14, height = 10, units = \"cm\")\n\n\n\n\n9.6.2 Test\nTo perform the Spearman rank correlation test, we can use the cor.test function with the method argument set to \"spearman\":\n\ncor_test_obj_spearman &lt;- cor.test(\n  data_tbl_error$x, data_tbl_error$y, method = \"spearman\"\n)\n\nHere are the results, which are quite messy:\n\n\nCode\ncor_test_obj_spearman\n\n\n\n    Spearman's rank correlation rho\n\ndata:  data_tbl_error$x and data_tbl_error$y\nS = 1834, p-value = 0.4054\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.1909091 \n\n\nTo extract the correlation and p-value, we can use the following code:\n\n\nCode\ncorr_spearman &lt;- cor_test_obj_spearman$estimate\ncorr_spearman\n\n\n       rho \n-0.1909091 \n\n\nCode\np_value_spearman &lt;- cor_test_obj_spearman$p.value\np_value_spearman\n\n\n[1] 0.4053961\n\n\nIt is both difficult and pointless to remember the exact syntax for extracting the correlation and p-value from the cor_test_obj_spearman object.\nOf course, one could always ask ChatGPT. Here’s the answer it gave me.\nBut typically it is a bit quicker to just look what is in the object and extract it. When you get complicated output like when printing the cor_test_obj_spearman object, you can use the following code to see what is in the object:\n\n\nCode\ncorr_test_obj_spearman |&gt; attributes()\n\n\n$names\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"  \n\n$class\n[1] \"htest\"\n\n\nIf we are not sure what these names mean exactly, typically they are listed in the help file of the function:\n\n\nCode\n?cor.test\n\n\nThey’re under the Value header (would need to scroll down a bit, comes right before examples):\n\nCode\nknitr::include_graphics(\"images/inference/help_file.png\")\n\n\n\ncorr_tbl_spearman &lt;- data.frame(\n  `est` = cor_test_obj_spearman$estimate,\n  `p_val` = cor_test_obj_spearman$p.value\n)\ncorr_tbl_spearman\n\n           est     p_val\nrho -0.1909091 0.4053961\n\n\nThis is not an attractive table. We can make it more presentable with the following code:\n\nCode\ncorr_tbl_spearman &lt;- corr_tbl_spearman |&gt;\n  # show only significant digits\n  mutate(\n    Correlation = est |&gt; signif(3),\n    `P-value` = p_val |&gt; signif(3)\n  ) |&gt;\n  dplyr::select(-c(est, p_val))\n# don't display the row names\nrownames(corr_tbl_spearman) &lt;- NULL\n# display using `kable` function:\ncorr_tbl_spearman |&gt; kable()\n\n\n\n\nCorrelation\nP-value\n\n\n\n\n-0.191\n0.405\n\n\n\nNote that for the kable function to produce good output, you need to have the chunk option results: asis set.\n\n\n9.6.3 Alternatives\nWe’ll talk about the Pearson correlation coefficient and the Concordance correlation coefficient next week.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#wilcoxon-rank-sum-test",
    "href": "hypothesistesting.html#wilcoxon-rank-sum-test",
    "title": "9  Hypothesis testing",
    "section": "9.7 Wilcoxon rank-sum test",
    "text": "9.7 Wilcoxon rank-sum test\nThe Wilcoxon rank-sum test is a non-parametric test that assesses whether two independent samples come from the same distribution.\nAs with the Spearman’s correlation coefficient, it is robust against outliers because it uses ranks instead of the actual values of the variables.\n\n9.7.1 Example\nSuppose that we have twenty samples from two groups:\n\n\nCode\nset.seed(4)\nx_vec &lt;- rnorm(20, 0, 1)\ny_vec &lt;- rnorm(20, 0.5, 1)\nsample_tbl_mw &lt;- tibble(group_1 = x_vec, group_2 = y_vec)\nsample_tbl_mw |&gt; head()\n\n\n# A tibble: 6 × 2\n  group_1 group_2\n    &lt;dbl&gt;   &lt;dbl&gt;\n1   0.217   2.04 \n2  -0.542   0.665\n3   0.891   1.81 \n4   0.596   1.79 \n5   1.64    1.09 \n6   0.689   0.217\n\n\nWe can compare if their medians (roughly speaking) are different using the Wilcoxon rank-sum test:\n\nwilcox_obj &lt;- wilcox.test(sample_tbl_mw$group_1, sample_tbl_mw$group_2)\nwilcox_obj\n\n\n    Wilcoxon rank sum exact test\n\ndata:  sample_tbl_mw$group_1 and sample_tbl_mw$group_2\nW = 145, p-value = 0.1417\nalternative hypothesis: true location shift is not equal to 0\n\n\nAgain, we don’t remember where the output is:\n\n\nCode\nwilcox_obj |&gt; attributes()\n\n\n$names\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"null.value\"  \"alternative\"\n[6] \"method\"      \"data.name\"  \n\n$class\n[1] \"htest\"\n\n\nWe extract and format the p-value:\n\n\nCode\nwilcox_obj[[\"p.value\"]] |&gt; signif(3)\n\n\n[1] 0.142",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#paired-data",
    "href": "hypothesistesting.html#paired-data",
    "title": "9  Hypothesis testing",
    "section": "9.8 Paired data",
    "text": "9.8 Paired data\nWhen we have paired data, we typically have much more power to detect differences.\nFor example, suppose we have two measurements from each of twenty people, pre- and post-treatment:\n\n\nCode\nset.seed(4)\nbase_vec &lt;- runif(20, 0, 5)\npre_vec &lt;- base_vec + rnorm(20, 0, 0.5)\npost_vec &lt;- base_vec + rnorm(20, 1, 0.5)\npaired_tbl &lt;- tibble(\n  pre = pre_vec,\n  post = post_vec\n)\npaired_tbl |&gt; head()\n\n\n# A tibble: 6 × 2\n     pre  post\n   &lt;dbl&gt; &lt;dbl&gt;\n1 3.21    4.01\n2 0.0526  1.57\n3 1.66    2.09\n4 1.36    1.65\n5 4.09    5.50\n6 1.39    2.10\n\n\nIf we perform the (unpaired) Mann-Whitney, we don’t find a significant p-value:\n\nwilcox_obj &lt;- wilcox.test(paired_tbl$pre, paired_tbl$post)\nwilcox_obj$p.value |&gt; signif(3)\n\n[1] 0.201\n\n\nBut if we use the paired-test equivalent, it is highly significant:\n\n\nCode\nwilcox_obj_paired &lt;- wilcox.test(paired_tbl$pre, paired_tbl$post, paired = TRUE)\nwilcox_obj_paired$p.value |&gt; signif(3)\n\n\n[1] 0.000261\n\n\nThe reason is that a lot of the “noise” (variability apart from the treatment) is removed when we use paired data. Sources of such variability include sex, age, income, etc.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#kruskal-wallis-test",
    "href": "hypothesistesting.html#kruskal-wallis-test",
    "title": "9  Hypothesis testing",
    "section": "9.9 Kruskal-Wallis test",
    "text": "9.9 Kruskal-Wallis test\nThe Kruskal-Wallis test is a non-parametric test that assesses whether three or more independent samples come from the same distribution.\nIt is a direct extension of the Mann-Whitney test to multiple groups.\n\n9.9.1 Example\nSuppose that we have add a third group to the previous example:\n\n\nCode\nset.seed(4)\nsample_tbl_kw &lt;- sample_tbl_mw |&gt;\n  mutate(\n    group_3 = rnorm(20, 4, 1)\n  )\nsample_tbl_kw |&gt; head()\n\n\n# A tibble: 6 × 3\n  group_1 group_2 group_3\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1   0.217   2.04     4.22\n2  -0.542   0.665    3.46\n3   0.891   1.81     4.89\n4   0.596   1.79     4.60\n5   1.64    1.09     5.64\n6   0.689   0.217    4.69\n\n\nWe can compare if their medians (roughly speaking) are different using the Kruskal-Wallis test:\n\nkw_obj &lt;- kruskal.test(\n  list(\n    sample_tbl_kw$group_1,\n    sample_tbl_kw$group_2,\n    sample_tbl_kw$group_3\n  )\n)\n\nAgain, we can extract and format the p-value:\n\n\nCode\nkw_obj[[\"p.value\"]] |&gt; signif(3)\n\n\n[1] 1.74e-09",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#multiple-testing",
    "href": "hypothesistesting.html#multiple-testing",
    "title": "9  Hypothesis testing",
    "section": "9.10 Multiple testing",
    "text": "9.10 Multiple testing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesistesting.html#homework",
    "href": "hypothesistesting.html#homework",
    "title": "9  Hypothesis testing",
    "section": "9.11 Homework",
    "text": "9.11 Homework\n\nInstall the package DataTidyRodoSTA2005S:\n\n\ninstall_github(\"MiguelRodo/DataTidyRodoSTA2005S@410a5247d600ce462e7941065233b634574fca86\")\n\n\ndata(\"data_tidy_yield\", package = \"DataTidy23RodoSTA2005SAssignment\")\ndata_tidy_yield\n\n# A tibble: 125 × 13\n   FarmID  CropYield Rainfall TempC TempF Fertilizer   Sunlight SoilPH WindSpeed\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Farm001      23.1    150.   19.2  66.5 CornCare        10.7    4.25      2.72\n 2 Farm002      28.6     86.9  16.5  61.7 KernelKindle     8.28   3.21      8.71\n 3 Farm003      27.5    107.   20.1  68.2 CornCare        10.7    4.61      6.64\n 4 Farm004      28.1    141.   29.9  85.8 CornCare        13.0    4.03      1.17\n 5 Farm005      28.0     85.0  18.6  65.6 KernelKindle     7.05   5.69      1.40\n 6 Farm006      27.6    102.   20.2  68.3 CornCare        10.7    4.81      1.98\n 7 Farm007      29.5    184.   23.5  74.2 KernelKindle    10.7    3.25      2.67\n 8 Farm008      23.2    170.   28.5  83.3 KernelKindle     5.81   4.16      4.83\n 9 Farm009      28.1    185.   22.5  72.4 KernelKindle     7.05   3.42      4.27\n10 Farm010      26.1    167.   20.0  68.0 CornCare        10.7    3.04      8.68\n# ℹ 115 more rows\n# ℹ 4 more variables: DistanceToWater &lt;dbl&gt;, Altitude &lt;dbl&gt;,\n#   PesticideApplied &lt;chr&gt;, Irrigation &lt;chr&gt;\n\n\nIt contains (simulated) data on maize crop yield (CropYield) under various conditions.\n\nApply the appropriate test and display the results in a table for the following questions:\n\n\nQuestion 1. Does crop yield depend on whether pesticide was applied?\nQuestion 2. Does crop yield depend on the irrigation type?\nQuestion 3. Does crop yield depend on rainfall?\nQuestion 4. Apply the Bonferroni multiple comparison correction to the results of Question 1-3.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "10  Inference",
    "section": "",
    "text": "11 Correlation\nWhat does correlation measure?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "correlation.html#different-strokes-for-different-folks",
    "href": "correlation.html#different-strokes-for-different-folks",
    "title": "10  Inference",
    "section": "11.1 Different strokes for different folks",
    "text": "11.1 Different strokes for different folks\n\n\nCode\n# generate data for each scenario\nset.seed(4)\n# Spearman: non-linear monotonic relationship\nx_spearman &lt;- runif(100, 0, 10)\ny_spearman &lt;- (x_spearman + 1)^5 + rnorm(100, 0, 3e3)\n\nset.seed(123)\n# Pearson: linear relationship\nx_pearson &lt;- runif(100, 0, 10)\ny_pearson &lt;- 2 * x_pearson + rnorm(100)\nx_example &lt;- x_pearson\ny_example &lt;- y_pearson\n\n\n# Concordance: linear relationship on y = x\nx_concordance &lt;- runif(100, 0, 10)\ny_concordance &lt;- x_concordance + rnorm(100, 0, 1)\n\n# Non-monotonic relationship\nx_nonmono &lt;- runif(100, 0, 10)\ny_nonmono &lt;- sin(x_nonmono) + rnorm(100, 0, 0.2)\n\ndata_spearman &lt;- tibble(\n  x = x_spearman, y = y_spearman, scenario = \"monotonic\"\n  )\ndata_pearson &lt;- tibble(\n  x = x_pearson, y = y_pearson, scenario = \"linear\"\n)\ndata_concordance &lt;- tibble(\n  x = x_concordance, y = y_concordance, scenario = \"matching\"\n)\ndata_nonmono &lt;- tibble(\n  x = x_nonmono, y = y_nonmono, scenario = \"non-monotonic\"\n)\nscenario_tbl &lt;- data_spearman |&gt;\n  dplyr::bind_rows(\n    data_pearson,\n    data_concordance,\n    data_nonmono\n  )\n\n\n\n\nCode\np_list &lt;- lapply(unique(scenario_tbl$scenario), function(scenario) {\n  data &lt;- scenario_tbl |&gt;\n    dplyr::filter(scenario == .env$scenario) |&gt;\n    dplyr::mutate(id = as.character(seq_len(dplyr::n()))) |&gt;\n    dplyr::mutate(y = y / sd(y), x = x / sd(x)) |&gt;\n    tidyr::pivot_longer(\n      cols = c(x, y),\n      names_to = \"grp\",\n      values_to = \"value\"\n    ) \n  if (scenario %in% c(\"linear\", \"monotonic\")) {\n    data &lt;- data |&gt;\n      dplyr::mutate(value = ifelse(grp == \"y\", value / 2, value))\n  }\n  UtilsGGSV::ggcorr(\n    data = data,\n    grp = \"grp\",\n    y = \"value\",\n    corr_method = c(\"spearman\", \"pearson\", \"concordance\"),\n    id = \"id\",\n    thm = theme_cowplot_custom(),\n    abline = TRUE,\n    grp_to_col = \"dodgerblue\",\n    skip = 0.07,\n    font_size = 3.5,\n    limits_equal = TRUE,\n    est_signif = 2,\n    pval_signif = 2,\n    ci_signif = 2,\n    point_alpha = 0.75\n  ) +\n  coord_equal() +\n  labs(title = switch(scenario,\n    monotonic = \"Monotonic\",\n    linear = \"Linear\",\n    matching = \"Matching\",\n    \"non-monotonic\" = \"Non-monotonic\"\n  ))\n})\np_grid &lt;- cowplot::plot_grid(\n  plotlist = p_list,\n  ncol = 2,\n  align = \"hv\"\n) +\n  theme(\n    panel.background = element_rect(fill = \"white\")\n  )\ncowplot::ggsave2(\n  filename = file.path(path_dir_fig, \"p-correlation_scenarios.png\"),\n  plot = p_grid,\n  width = 20,\n  height = 20,\n  units = \"cm\"\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "correlation.html#relationship-to-inference",
    "href": "correlation.html#relationship-to-inference",
    "title": "10  Inference",
    "section": "11.2 Relationship to inference",
    "text": "11.2 Relationship to inference\n\nHypothesis testing: we can check if the data are compatible with the correlation being zero\nConfidence intervals: we can estimate a range of plausible values for the correlation",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "correlation.html#correlation-estimation-and-inference-in-r",
    "href": "correlation.html#correlation-estimation-and-inference-in-r",
    "title": "10  Inference",
    "section": "11.3 Correlation estimation and inference in R",
    "text": "11.3 Correlation estimation and inference in R\n\n11.3.1 Spearman and Pearson\nIn this case, we use the cor.test function (available by default in R), as we did last week:\n\ncor.test(x = x_pearson, y = y_pearson)\n\n\n    Pearson's product-moment correlation\n\ndata:  x_pearson and y_pearson\nt = 58.247, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9790245 0.9904827\nsample estimates:\n     cor \n0.985863 \n\n\nBy default, the wilcox.test function uses the Pearson correlation:\n\ncor.test(x = x_example, y = y_example, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  x_example and y_example\nt = 58.247, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9790245 0.9904827\nsample estimates:\n     cor \n0.985863 \n\n\nWe can specify the method to use the Spearman correlation:\n\ncor.test(x = x_example, y = y_example, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  x_example and y_example\nS = 2296, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9862226 \n\n\nThe Spearman method lacks a confidence interval, in this case.\n\n\n11.3.2 Concordance correlation coefficient\nTo compute the concordance correlation coefficient, we can use the cccUst function from the cccrm package:\n\nccc_tbl &lt;- tibble(\n  value = c(x_example, y_example),\n  grp = rep(c(\"x\", \"y\"), each = length(x_example))\n)\nccc_obj &lt;- cccUst(\n  dataset = ccc_tbl, ry = \"value\", rmet = \"grp\", cl = 0.95\n)\nccc_obj\n\nCCC estimated by U-statistics: \n       CCC  LL CI 95%  UL CI 95%     SE CCC \n0.49139477 0.44480249 0.53533453 0.02310291 \n\n\nIt lacks a p-value, however.\nWe can also use the CCC function from the DescTools package:\n\nCCC(x_example, y_example)$rho.c\n\n        est    lwr.ci    upr.ci\n1 0.4913948 0.4194006 0.5572464\n\n\nThe confidence intervals, interestingly, are clearly narrower than for the cccrm package.\n\n11.3.2.1 Comparing confidence interval coverage\nLet’s compare the actual coverage percentages.\nSo we’ll see what percentage of time the confidence intervals actually contain the true correlation, across sample sizes and methods and nature of relationship.\n\n11.3.2.1.1 Key functions\nFirst, we define a function to calculate the concordance correlation coefficient on large datasets:\n\n\nCode\ncalc_ccc &lt;- function(x, y, n_test = 1e5) {\n  n_test &lt;- 1e5\n  n_est &lt;- length(x) / n_test\n  est_sum &lt;- rep(0, n_est)\n  lb_sum &lt;- rep(0, n_est)\n  ub_sum &lt;- rep(0, n_est)\n  for (i in seq_len(n_est)) {\n    ind_vec &lt;- seq((i - 1) * n_test + 1, n_test * i)\n    ccc_obj &lt;- CCC(x[ind_vec], y[ind_vec])$rho.c\n    est_sum &lt;- sum(est_sum, ccc_obj[[1]])\n    lb_sum &lt;- sum(lb_sum, ccc_obj[[2]])\n    ub_sum &lt;- sum(ub_sum, ccc_obj[[3]])\n  }\n  ccc_obj &lt;- c(est_sum, lb_sum, ub_sum) / n_est\n  ccc_obj\n}\n\n\nThen we’ll find a function to simulate the coverage:\n\n\nCode\nsimulate_coverage &lt;- function(x,\n                              y,\n                              lb,\n                              ub,\n                              seed,\n                              n_boot = 1e3, \n                              method = NULL,\n                              sample_size = NULL) {\n  method &lt;- if (is.null(method)) {\n    c(\"cccrm\", \"z-transform\", \"asymptotic\")\n  } else {\n    method\n  }\n  sample_size &lt;- if (is.null(sample_size)) {\n    c(5, 10, 20, 50, 100, 200, 500, 1e3, 2e3, 5e3, 1e4)[1:4]\n  } else {\n    sample_size\n  }\n  set.seed(seed)\n  sample_size_ind &lt;- sample_size[1]; method_ind &lt;- \"z-transform\"\n  purrr::map_df(sample_size, function(sample_size_ind) {\n    print(sample_size_ind)\n    purrr::map_df(method, function(method_ind) {\n      print(method_ind)\n      inc_vec &lt;- purrr::map_lgl(seq_len(n_boot), function(i) {\n        ind_vec &lt;- seq((i - 1) * sample_size_ind + 1, sample_size_ind * i)\n        boot_vec_x &lt;- x[ind_vec]\n        boot_vec_y &lt;- y[ind_vec]\n        boot_tbl &lt;- tibble(\n          value = c(boot_vec_x, boot_vec_y),\n          grp = rep(c(\"x\", \"y\"), each = sample_size_ind)\n        )\n        ci &lt;- if (method_ind == \"cccrm\") {\n          cccUst(\n            dataset = boot_tbl,\n            ry = \"value\",\n            rmet = \"grp\",\n            cl = 0.95\n          )[2:3]\n        } else {\n          CCC(boot_vec_x, boot_vec_y, ci = method_ind)$rho.c[2:3] |&gt; unlist()\n        }\n        ci_vec &lt;- seq(ci[1], ci[2], length.out = 1e2)\n        any(ci_vec &gt;= lb & ci_vec &lt;= ub)\n      })\n      tibble::tibble(\n        sample_size = sample_size_ind, method = method_ind, coverage = mean(inc_vec)\n      )\n    })\n  })\n}\n\n\n\n\n11.3.2.1.2 Calculation\nWe’ll now calculate covarage, under three scenarios:\n\nA matching relationship\nA linear but non-matching relationship\nA monotonic but non-linear relationship\n\n\n11.3.2.1.2.1 Matching\nFirst, let’s generate a large dataset, where can “know” the true correlation:\n\n\nCode\nset.seed(4)\nx_all_match &lt;- runif(n_obs_all, 0, 10)\ny_all_match &lt;- x_all_match + rnorm(n_obs_all, 0, 1)\nccc_vec_match &lt;- calc_ccc(x_all_match, y_all_match)\nccc_vec_match |&gt; signif(4)\n\n\nSo we know where the correlation coefficient lies quite precisely. So we’ll count any confidence interval that overlaps with this as correct.\n\n\nCode\nresults_tbl_match &lt;- simulate_coverage(\n  x = x_all_match,\n  y = y_all_match,\n  lb = ccc_vec_match[[2]],\n  ub = ccc_vec_match[[3]],\n  seed = 4,\n  n_boot = n_boot\n)\n\n\n\n\n\n11.3.2.1.3 Linear\nFirst, let’s generate a large dataset, where we can “know” the true correlation:\n\n\nCode\nset.seed(4)\nn_obs_all &lt;- 1e6\nx_all_linear &lt;- runif(n_obs_all, 0, 10)\ny_all_linear &lt;- 2 * x_all_linear + rnorm(100)\nccc_vec_linear &lt;- calc_ccc(x_all_linear, y_all_linear)\nccc_vec_linear |&gt; signif(4)\n\n\nSo we know where the correlation coefficient lies quite precisely. So we’ll count any confidence interval that overlaps with this as correct.\n\n\nCode\nresults_tbl_linear &lt;- simulate_coverage(\n  x = x_all_linear,\n  y = y_all_linear,\n  lb = ccc_vec_linear[[2]],\n  ub = ccc_vec_linear[[3]],\n  seed = 4,\n  n_boot = n_boot\n)\n\n\n\n\n11.3.2.1.4 Monotonic\n\n\nCode\nset.seed(4)\nn_obs_all &lt;- 1e6\n# Spearman: monotonic monotonic relationship\nx_all_monotonic &lt;- runif(n_obs_all, 0, 10)\ny_all_monotonic &lt;- (x_all_monotonic + 1)^5 + rnorm(n_obs_all, 0, 3e3)\ny_all_monotonic &lt;- y_all_monotonic / sd(y_all_monotonic) / 2\nccc_vec_monotonic &lt;- calc_ccc(x_all_monotonic, y_all_monotonic)\nccc_vec_monotonic |&gt; signif(4)\n\n\nSo we know where the correlation coefficient lies quite precisely. So we’ll count any confidence interval that overlaps with this as correct.\n\n\nCode\nresults_tbl_monotonic &lt;- simulate_coverage(\n  x = x_all_monotonic,\n  y = y_all_monotonic,\n  lb = ccc_vec_monotonic[[2]],\n  ub = ccc_vec_monotonic[[3]],\n  seed = 4,\n  n_boot = n_boot\n)\n\n\n\n\n\n11.3.2.2 Results\nHere is a plot of the results:\n\n\nCode\nplot_tbl &lt;- results_tbl_match |&gt;\n  dplyr::mutate(scenario = \"Matching\") |&gt;\n  dplyr::bind_rows(\n    results_tbl_linear |&gt;\n      dplyr::mutate(scenario = \"Linear\")\n  ) |&gt;\n  dplyr::bind_rows(\n    results_tbl_monotonic |&gt;\n      dplyr::mutate(scenario = \"Monotonic\")\n  ) |&gt;\n  dplyr::mutate(\n    sample_size_num = case_when(\n      sample_size == 5 ~ 1,\n      sample_size == 10 ~ 2,\n      sample_size == 20 ~ 3,\n      sample_size == 50 ~ 4\n    )\n  )\np &lt;- ggplot(\n  plot_tbl,\n  aes(x = sample_size_num, y = coverage, color = method)\n) +\n  geom_line(aes(color = method), lwd = 1.2) +\n  geom_point(colour = \"gray50\") +\n  geom_hline(yintercept = 0.95, linetype = \"dashed\") +\n  cowplot::theme_cowplot() +\n  cowplot::background_grid(major = \"xy\") +\n  theme(plot.background = element_rect(fill = \"white\")) +\n  theme(panel.background = element_rect(fill = \"white\")) +\n  scale_x_continuous(labels = c(5, 10, 20, 50)) +\n  facet_wrap(~scenario) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"gray50\")\n  ) +\n  labs(x = \"Coverage\", y = \"Sample size\") +\n  scale_colour_manual(\n    values = c(\"cccrm\" = \"#66c2a5\", \"z-transform\" = \"#fc8d62\", \"asymptotic\" = \"#8da0cb\"),\n    name = \"Method\"\n  )\ncowplot::ggsave2(\n  filename = file.path(path_dir_fig, \"p-correlation_coverage.png\"),\n  plot = p,\n  width = 15,\n  height = 8,\n  units = \"cm\"\n)\n\n\n\nCode\nknitr::include_graphics(file.path(path_dir_fig, \"p-correlation_coverage.png\"))\n\n\n\n\n\n11.3.3 Conclusion\nAlways use the DescTools package for confidence interval estimation, as it’s coverage is practically useful and consistently better than the cccrm package. Either the z-transform or asymptotic methods are fine.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "correlation.html#plotting-correlation-coefficients",
    "href": "correlation.html#plotting-correlation-coefficients",
    "title": "10  Inference",
    "section": "11.4 Plotting correlation coefficients",
    "text": "11.4 Plotting correlation coefficients\n\n11.4.1 Straight ggplot2\nWe can plot the raw data, calculate the coefficient and add it manually:\n\n\n11.4.2 ggpubr\n\n\nCode\n# Load data\ndata(\"mtcars\")\ndf &lt;- mtcars\n\n# Scatter plot with correlation coefficient\n#:::::::::::::::::::::::::::::::::::::::::::::::::\nsp &lt;- ggscatter(df, x = \"wt\", y = \"mpg\",\n   add = \"reg.line\",  # Add regressin line\n   add.params = list(color = \"blue\", fill = \"lightgray\"), # Customize reg. line\n   conf.int = TRUE # Add confidence interval\n   )\n# Add correlation coefficient\nsp + stat_cor(method = \"pearson\", label.x = 3, label.y = 30)\n\n\n\n\n\n\n\n\n\nIt does not support the concordance correlation coefficient, however.\n\n\n11.4.3 UtilsGGSV::ggcorr\nThis is the function I’ve developed over the years to help with quality assurance at SATVI. When we want to check whether two people running the same assay on the same participants get the same result, we use the concordance correlation coefficient. However, it can also do the Spearman and Pearson correlation coefficients.\nInstall it using the following:\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  utils::install.packages(\"remotes\")\n}\nremotes::install_github(\"SATVILab/UtilsGGSV\")\n\nThe function ggcorr plots correlation coefficients (see ?ggcorr for more information):\n\n\nCode\nset.seed(3)\nresponse_vec_a &lt;- rnorm(5)\nresponse_tbl &lt;- data.frame(\n  group = rep(letters[1:3], each = 5),\n  response = c(\n    response_vec_a,\n    response_vec_a * 1.2 + rnorm(5, sd = 0.2),\n    response_vec_a * 2 + rnorm(5, sd = 2)\n  ),\n  pid = rep(paste0(\"id_\", 1:5), 3)\n)\n\nggcorr(\n  data = response_tbl |&gt; dplyr::filter(group %in% c(\"a\", \"b\")),\n  grp = \"group\",\n  y = \"response\",\n  id = \"pid\"\n)\n\n\n\n\n\n\n\n\n\nWe can display multiple correlation coefficients:\n\n\nCode\nggcorr(\n  data = response_tbl |&gt; dplyr::filter(group %in% c(\"a\", \"b\")),\n  grp = \"group\",\n  y = \"response\",\n  id = \"pid\",\n  corr_method = c(\"spearman\", \"pearson\")\n)\n\n\n\n\n\n\n\n\n\nWe can compare more than two groups:\n\n\nCode\nggcorr(\n  data = response_tbl,\n  grp = \"group\",\n  y = \"response\",\n  id = \"pid\",\n  corr_method = \"kendall\"\n)\n\n\n\n\n\n\n\n\n\nWe can compare more than two groups and multiple correlation coefficients:\n\n\nCode\nggcorr(\n  data = response_tbl,\n  grp = \"group\",\n  y = \"response\",\n  id = \"pid\",\n  corr_method = c(\"spearman\", \"pearson\")\n)\n\n\n\n\n\n\n\n\n\nSpecific functionality to make appropriate plots for the concordance correlation coefficient is available:\n\n\nCode\nggcorr(\n  data = response_tbl |&gt; dplyr::filter(group %in% c(\"a\", \"b\")),\n  grp = \"group\",\n  y = \"response\",\n  id = \"pid\",\n  corr_method = \"concordance\",\n  abline = TRUE,\n  limits_equal = TRUE\n)\n\n\n\n\n\n\n\n\n\nText in table can be moved around and resized:\n\n\nCode\nggcorr(\n  data = response_tbl |&gt; dplyr::filter(group %in% c(\"a\", \"b\")),\n  grp = \"group\",\n  y = \"response\",\n  id = \"pid\",\n  corr_method = c(\"spearman\", \"pearson\", \"concordance\"),\n  abline = TRUE,\n  limits_equal = TRUE,\n  coord = c(0.4, 0.17),\n  font_size = 3,\n  skip = 0.04,\n  pval_signif = 2,\n  est_signif = 2,\n  ci_signif = 2\n)\n\n\n\n\n\n\n\n\n\nFinally, the text placement is kept consistent when the axes are visually transformed:\n\n\nCode\nggcorr(\n  data = response_tbl |&gt; dplyr::mutate(response = abs(response + 1)^4),\n  grp = \"group\",\n  y = \"response\",\n  id = \"pid\",\n  corr_method = \"spearman\",\n  abline = TRUE,\n  limits_equal = TRUE,\n  trans = \"log10\",\n  skip = 0.06\n)\n\n\n\n\n\n\n\n\n\n\n\n11.4.4 Bootstrapping\nMaybe next time!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "correlation.html#homework",
    "href": "correlation.html#homework",
    "title": "10  Inference",
    "section": "11.5 Homework",
    "text": "11.5 Homework\n\n11.5.1 Question one\nA lab wants to check that operator B, a trainee, can achieve the same results on the same assay as operator A. Read in the data by running the following command:\n\noperator_tbl &lt;- structure(list(operator = c(\"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \n\"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \n\"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \n\"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \n\"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \n\"B\"), measurement = c(5.86, 0.0895, 2.94, 2.77, 8.14, 2.6, 7.24, \n9.06, 9.49, 0.731, 7.55, 2.86, 1, 9.54, 4.16, 4.55, 9.71, 5.84, \n9.62, 7.62, 7.15, 9.97, 5.06, 4.9, 6.49, 8.31, 4.82, 8.42, 5.14, \n5.3, 7.78, 1.28, 3.77, 3.51, 10.3, 4.93, 9.58, 13.1, 13.6, 1.54, \n9.53, 4.97, 2.21, 11.5, 6.64, 6.07, 13.7, 6.84, 11, 10.8, 8.88, \n12.7, 7.52, 5.9, 7.8, 12.1, 6.45, 12.2, 4.99, 6.07)), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"), row.names = c(NA, -60L))\n\n\nCalculate the appropriate correlation coefficient.\nPlot the raw data and the appropriate correlation coefficient.\nInterpret the results.\n\n\n\n11.5.2 Question two\nWe are interested in knowing whether two genes are associated. We do not know the nature of the relationship, if there is one.\nRead the data in, as follows:\n\ngene_tbl &lt;- structure(list(gene = c(\"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \n\"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \n\"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \n\"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \"UNDERMINER4\", \n\"UNDERMINER4\", \"UNDERMINER4\", \"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \n\"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \n\"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \n\"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \"TROGLODYTE7\", \n\"TROGLODYTE7\", \"TROGLODYTE7\"), expression = c(5.85800305008888, \n0.0894579570740461, 2.93739611981437, 2.77374957920983, 8.13574214931577, \n2.60427771368995, 7.24405892658979, 9.06092151300982, 9.49040221050382, \n0.73144469410181, 7.54675026983023, 2.8600062080659, 1.00053521571681, \n9.5406877505593, 4.15607118513435, 4.55102417618036, 9.71055655973032, \n5.83987979684025, 9.6220462443307, 7.6170240319334, 201.591007212395, \n0.0164353615310555, 25.7277804258383, 21.2952232089115, 538.541568097937, \n17.8319216170362, 381.30708631164, 743.86015840029, 854.678654517749, \n0.107886637441148, 431.354200915445, 23.558977358528, 2.3092288669259, \n869.726714251939, 72.3804140205039, 93.9770546568619, 916.911928248853, \n200.074244601714, 889.917326105173, 443.17271786801)), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"), row.names = c(NA, -40L))\n\n\nCalculate the most appropriate correlation coefficient. Is it statistically significant?\nPlot the raw data and the appropriate correlation coefficient.\nInterpret the results.\n\n\n\n11.5.3 Question three\nWe have been reliably informed that two variables are linearly related (last question imagination levels here). Read the data in as follows:\n\n\nCode\nset.seed(4)\nn_obs &lt;- 15\nx_all_match &lt;- runif(n_obs, 0, 10)\ny_all_match &lt;- 5 + x_all_match * 10 + rnorm(n_obs, 0, 10)\nvar_tbl &lt;- tibble(\n  variable = rep(c(\"exciting-thing-1\", \"exciting-thing-2\"), each = n_obs),\n  value = c(x_all_match, y_all_match)\n)\ndput(var_tbl)\n\n\nstructure(list(variable = c(\"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \n\"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \n\"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \n\"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\"\n), value = c(5.85800305008888, 0.0894579570740461, 2.93739611981437, \n2.77374957920983, 8.13574214931577, 2.60427771368995, 7.24405892658979, \n9.06092151300982, 9.49040221050382, 0.73144469410181, 7.54675026983023, \n2.8600062080659, 1.00053521571681, 9.5406877505593, 4.15607118513435, \n62.4522288284119, 8.01564342234369, 41.4918552015465, 59.8149071489473, \n86.1053092224815, 40.6163456528487, 87.4571949887254, 96.3563217676658, \n92.8071441398129, 16.2900278303966, 99.4289883901294, 36.7089255281769, \n-9.78185363090419, 93.4135192204788, 37.4412720963096)), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"), row.names = c(NA, -30L))\n\n\nRead the data in as follows:\n\nvar_tbl &lt;- structure(list(variable = c(\"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \"exciting-thing-1\", \n\"exciting-thing-1\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \n\"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \n\"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \n\"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\", \"exciting-thing-2\"\n), value = c(5.85800305008888, 0.0894579570740461, 2.93739611981437, \n2.77374957920983, 8.13574214931577, 2.60427771368995, 7.24405892658979, \n9.06092151300982, 9.49040221050382, 0.73144469410181, 7.54675026983023, \n2.8600062080659, 1.00053521571681, 9.5406877505593, 4.15607118513435, \n62.4522288284119, 8.01564342234369, 41.4918552015465, 59.8149071489473, \n86.1053092224815, 40.6163456528487, 87.4571949887254, 96.3563217676658, \n92.8071441398129, 16.2900278303966, 99.4289883901294, 36.7089255281769, \n-9.78185363090419, 93.4135192204788, 37.4412720963096)), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"), row.names = c(NA, -30L))\n\n\nCalculate the most appropriate correlation coefficient. Is it statistically significant?\nPlot the raw data and the appropriate correlation coefficient.\nInterpret the results.\nCongratulate yourself for uncovering this important relationship!\n\n\n\n11.5.4 Question four\nComplete that blank section detailing how to plot the correlation coefficient using ggplot2 directly above. Bonus marks (and immortality) for contributing the answer via GitHub to the repo (https://github.com/SATVILab/SATVI_ComputationalCourse).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "automatedGatingTraining.html",
    "href": "automatedGatingTraining.html",
    "title": "11  Automated gating of flow Cytometry Data in R",
    "section": "",
    "text": "This is an example script for running the automated gating pipeline to obtain frequencies of antigen-specific CD4 and CD8 T cells.\nContact Munya Musvosvi at munyaradzi.musvovi@uct.ac.za for assistance, or log issues at UtilsAutoGating.\n\nlibrary(cytoUtils)\nlibrary(flowCore)#This packages are not being imported for some reason\nlibrary(openCyto)#This packages are not being imported for some reason\nlibrary(ggcyto)#This packages are not being imported for some reason\nlibrary(magick)\nlibrary(CytoML)\nlibrary(rio)\nlibrary(PeacoQC)\n#devtools::install_github(\"RGLab/cytoUtils\")\n#BiocManager::install(c(\"SummarizedExperiment\", \"SingleCellExperiment\", \"TreeSummarizedExperiment\"))\n\nfcs_location = \"\"#absolute path to the folder with the fcs files and the FlowJo workspace\nresults_location = \"\"#absolute path of the folder you want to save the analysed data\n\n  fcs_files = list.files(fcs_location,full.names = T,pattern = \".fcs\")\n  fcs_files = fcs_files[!grepl(\"Compensation Controls\",fcs_files,fixed = T)]\n  fcs_files = fcs_files[grepl(\".fcs\",fcs_files,fixed = T)]\n  wsp_location = list.files(fcs_location,full.names = T,pattern = \".wsp\")\n  if(length(wsp_location)&gt;1){\n    wsp_location = wsp_location[order(file.info(wsp_location)$mtime,decreasing = T)][1]#Selects the most up to date .wsp\n  }\n\n  fcs_temp = load_cytoset_from_fcs(fcs_files[1])\n  markers_in_fcs_files = as.data.frame(markernames(fcs_temp))\n  colnames(markers_in_fcs_files) = \"marker\"\n\n  base_lineage_gating =data.frame(\n    alias = c(\"boundary\", \"singlets\", \"SSCneg\", \"lymph\", \"cd3.gate\", \"cd8.neg\", \"cd4.neg\", \"cd4.gate\", \"cd8.gate\"),\n    pop = c(\"+\", \"+\", \"-\", \"+\", \"+\", \"-\", \"-\", \"+\", \"+\"),\n    parent = c(\"root\", \"boundary\", \"singlets\", \"SSCneg\", \"lymph\", \"cd3.gate\", \"cd3.gate\", \"cd8.neg\", \"cd4.neg\"),\n    dims = c(\"FSC-A,FSC-H\", \"FSC-A,FSC-H\", \"SSC-A,\", \"FSC-A,SSC-A\", \"CD3\", \"CD8\", \"CD4\", \"CD4\", \"CD8\"),\n    gating_method = c(\"boundary\", \"singletGate\", \"gate_mindensity2\", \"gate_flowclust_2d\", \"gate_mindensity2\", \"gate_mindensity2\", \"gate_mindensity2\", \"gate_mindensity2\", \"gate_mindensity2\"),\n    gating_args = c(\"max=c(256000,256000),min=c(20000,20000)\", \"\", \"peaks=2,min=30000,max=230000,gate_range = c(40000,200000)\", \"K=1,quantile=.9\", \"min=0.5,max=3.5\", \"peaks= 2,min =0.5,max=3.5\", \"peaks= 2,min =0.5,max=3.5\", \"peaks= 2, min=0.5, max=3.5,gate_range=c(1,3)\", \"peaks= 2, min=0.5, max=3.5,gate_range=c(1,3)\"),\n    collapseDataForGating = rep(TRUE, 9),\n    groupBy = rep(\"pid\", 9),\n    preprocessing_method = c(\"ppmyGate\", \"ppmyGate\", \"ppmyGate\", NA, \"ppmyGate\", \"ppmyGate\", \"ppmyGate\", \"ppmyGate\", \"ppmyGate\"),\n    preprocessing_args = rep(NA, 9),\n    stringsAsFactors = FALSE)\n\n  if(sum(markers_in_fcs_files$marker%in%c(\"IFNG\",\"IFNg\",\"IFN-g\",\"IFNy\",\"IFN-y\",\"IFN-gamma\",\"IFNgamma\"))==1){\n    IFNG = \"yes\"\n  }else{\n    IFNG = \"no\"\n  }\n\n  if(sum(markers_in_fcs_files$marker%in%c(\"IL2\",\"IL-2\"))==1){\n    IL2 = \"yes\"\n  }else{\n    IL2 = \"no\"\n  }\n\n  if(sum(markers_in_fcs_files$marker%in%c(\"TNF\",\"TNF-a\",\"TNFa\",\"TNF-alpha\",\"TNFalpha\"))==1){\n    TNF = \"yes\"\n  }else{\n    TNF = \"no\"\n  }\n\n  if(sum(markers_in_fcs_files$marker%in%c(\"IL22\",\"IL-22\"))==1){\n    IL22 = \"yes\"\n  }else{\n    IL22 = \"no\"\n  }\n\n  if(sum(markers_in_fcs_files$marker%in%c(\"IL17\",\"IL-17\"))==1){\n    IL17 = \"yes\"\n  }else{\n    IL17 = \"no\"\n  }\n\n  if(IFNG==\"yes\"){\n    cytokine_IFNG = data.frame(\n      alias = c(\"cd4IFNG\", \"cd8IFNG\"),\n      pop = c(\"+\", \"+\"),\n      parent = c(\"cd4.gate\", \"cd8.gate\"),\n      dims = c(\"IFNG\", \"IFNG\"),\n      gating_method = c(\"gate_tail\", \"gate_tail\"),\n      gating_args = c(\"auto_tol = TRUE, bias = 0.3\", \"auto_tol = TRUE, bias = 0.3\"),\n      collapseDataForGating = c(TRUE, TRUE),\n      groupBy = c(\"pid\", \"pid\"),\n      preprocessing_method = c(\"ppmyGate\", \"ppmyGate\"),\n      preprocessing_args = c(NA, NA),\n      stringsAsFactors = FALSE)\n  }else{\n    cytokine_IFNG = NULL\n  }\n\n  if(TNF==\"yes\"){\n    cytokine_TNF = data.frame(\n      alias = c(\"cd4TNF\", \"cd8TNF\"),\n      pop = c(\"+\", \"+\"),\n      parent = c(\"cd4.gate\", \"cd8.gate\"),\n      dims = c(\"TNF\", \"TNF\"),\n      gating_method = c(\"gate_tail\", \"gate_tail\"),\n      gating_args = c(\"auto_tol = TRUE, bias = 0.3\", \"auto_tol = TRUE, bias = 0.3\"),\n      collapseDataForGating = c(TRUE, TRUE),\n      groupBy = c(\"pid\", \"pid\"),\n      preprocessing_method = c(\"ppmyGate\", \"ppmyGate\"),\n      preprocessing_args = c(NA, NA),\n      stringsAsFactors = FALSE)\n  }else{\n    cytokine_TNF = NULL\n  }\n\n  if(IL2==\"yes\"){\n    cytokine_IL2 = data.frame(\n      alias = c(\"cd4IL2\", \"cd8IL2\"),\n      pop = c(\"+\", \"+\"),\n      parent = c(\"cd4.gate\", \"cd8.gate\"),\n      dims = c(\"IL2\", \"IL2\"),\n      gating_method = c(\"gate_tail\", \"gate_tail\"),\n      gating_args = c(\"auto_tol = TRUE, bias = 0.3\", \"auto_tol = TRUE, bias = 0.3\"),\n      collapseDataForGating = c(TRUE, TRUE),\n      groupBy = c(\"pid\", \"pid\"),\n      preprocessing_method = c(\"ppmyGate\", \"ppmyGate\"),\n      preprocessing_args = c(NA, NA),\n      stringsAsFactors = FALSE)\n  }else{\n    cytokine_IL2 = NULL\n  }\n\n  if(IL17==\"yes\"){\n    cytokine_IL17 = data.frame(\n      alias = c(\"cd4IL17\", \"cd8IL17\"),\n      pop = c(\"+\", \"+\"),\n      parent = c(\"cd4.gate\", \"cd8.gate\"),\n      dims = c(\"IL17\", \"IL17\"),\n      gating_method = c(\"gate_tail\", \"gate_tail\"),\n      gating_args = c(\"auto_tol = TRUE, bias = 0.3\", \"auto_tol = TRUE, bias = 0.3\"),\n      collapseDataForGating = c(TRUE, TRUE),\n      groupBy = c(\"pid\", \"pid\"),\n      preprocessing_method = c(\"ppmyGate\", \"ppmyGate\"),\n      preprocessing_args = c(NA, NA),\n      stringsAsFactors = FALSE)\n  }else{\n    cytokine_IL17 = NULL\n  }\n\n  if(IL22==\"yes\"){\n    cytokine_IL22 = data.frame(\n      alias = c(\"cd4IL22\", \"cd8IL22\"),\n      pop = c(\"+\", \"+\"),\n      parent = c(\"cd4.gate\", \"cd8.gate\"),\n      dims = c(\"IL22\", \"IL22\"),\n      gating_method = c(\"gate_tail\", \"gate_tail\"),\n      gating_args = c(\"auto_tol = TRUE, bias = 0.3\", \"auto_tol = TRUE, bias = 0.3\"),\n      collapseDataForGating = c(TRUE, TRUE),\n      groupBy = c(\"pid\", \"pid\"),\n      preprocessing_method = c(\"ppmyGate\", \"ppmyGate\"),\n      preprocessing_args = c(NA, NA),\n      stringsAsFactors = FALSE)\n  }else{\n    cytokine_IL22 = NULL\n  }\n\n  cytokine_gating = rbind.data.frame(cytokine_IFNG,cytokine_TNF,cytokine_IL2,cytokine_IL17,cytokine_IL22)\n\n  cd4_boolean = data.frame(\n    alias = c(\"*\", \"cd4_total_cytokine\"),\n    pop = c(\"*\", \"+\"),\n    parent = c(\"cd4.gate\", \"cd4.gate\"),\n    dims = c(\"IFNg,TNF\", \"IFNg,TNF\"),\n    gating_method = c(\"polyFunctions\", \"boolGate\"),\n    gating_args = c(paste(paste(\"cd4.gate/\",subset(cytokine_gating,cytokine_gating$parent==\"cd4.gate\")$alias,sep = \"\"),collapse = \":\"), paste(paste(\"cd4.gate/\",subset(cytokine_gating,cytokine_gating$parent==\"cd4.gate\")$alias,sep = \"\"),collapse = \"|\")),\n    collapseDataForGating = c(TRUE, TRUE),\n    groupBy = c(\"pid\", \"pid\"),\n    preprocessing_method = c(\"ppmyGate\", \"ppmyGate\"),\n    preprocessing_args = c(NA, NA),\n    stringsAsFactors = FALSE)\n\n  cd8_boolean = data.frame(\n    alias = c(\"*\", \"cd8_total_cytokine\"),\n    pop = c(\"*\", \"+\"),\n    parent = c(\"cd8.gate\", \"cd8.gate\"),\n    dims = c(\"IFNg,TNF\", \"IFNg,TNF\"),\n    gating_method = c(\"polyFunctions\", \"boolGate\"),\n    gating_args = c(paste(paste(\"cd8.gate/\",subset(cytokine_gating,cytokine_gating$parent==\"cd8.gate\")$alias,sep = \"\"),collapse = \":\"), paste(paste(\"cd8.gate/\",subset(cytokine_gating,cytokine_gating$parent==\"cd8.gate\")$alias,sep = \"\"),collapse = \"|\")),\n    collapseDataForGating = c(TRUE, TRUE),\n    groupBy = c(\"pid\", \"pid\"),\n    preprocessing_method = c(\"ppmyGate\", \"ppmyGate\"),\n    preprocessing_args = c(NA, NA),\n    stringsAsFactors = FALSE)\n\n  temp_gating_strategy = rbind.data.frame(base_lineage_gating,cytokine_gating,cd4_boolean,cd8_boolean)\n\n  temp_file &lt;- tempfile(fileext = \".csv\")\n  write.csv(temp_gating_strategy, temp_file, row.names = FALSE)\n\n  gating_strategy &lt;- gatingTemplate(temp_file)#use\n  temp_gating &lt;- read.csv(temp_file)\n\n  metadata &lt;- NULL\n  for(i in 1:length(fcs_files)){\n    temp_meta = as.data.frame(fcs_files[i])\n    colnames(temp_meta) =\"file_location\"\n    temp_keywords =keyword(read.FCS(fcs_files[i]))\n    temp_meta$batch = temp_keywords$`$SRC`\n    temp_meta$stimulation = gsub(\"/\",\"_\",temp_keywords$`TUBE NAME`,fixed = T)\n    temp_meta$stimulation = gsub(\"\\\\\\\\\",\"_\",temp_meta$stimulation,fixed = T)\n    temp_meta$file_name = temp_keywords$`$FIL`\n    metadata = rbind.data.frame(metadata,temp_meta)\n  }\n\n  batches_to_analyse &lt;- unique(metadata$batch)\n\n  .ppmyGate &lt;- function(fs, gs, gm, channels=NA,groupBy=NA,isCollapse=NA, ...) {\n    xChannel = channels[1]\n    yChannel = channels[1]\n    d &lt;- c()\n    for(i in c(1:length(fs))) {\n      d &lt;- c(d,rep.int(pData(fs[i])$control,nrow(exprs(fs[[i]]))))\n    }\n    return(as.logical(d))\n  }\n  register_plugins(fun=.ppmyGate, methodName='ppmyGate', dep=NA, \"preprocessing\")\n\n  .polyGate &lt;- function(fr, pp_res, channels, filterId=\"polygate\", ...){\n    args &lt;- list(...)\n    g &lt;- data.frame(x=args$x, y=args$y)\n    colnames(g) &lt;- channels\n    flowCore::polygonGate(.gate=g, filterId=filterId)\n  }\n  register_plugins(fun=.polyGate, methodName='polyGate', dep=NA)\n\n  .myGate &lt;- function(fr, pp_res, channels=NA, filterId=\"ppgate\", ...){\n    my_gate &lt;- tailgate(fr[pp_res,],channel=channels, filter_id=filterId, ...)\n    return(my_gate)\n  }\n  register_plugins(fun=.myGate,methodName='myGate',dep=NA)\n\n\n  for(temp_sampleBatch in batches_to_analyse){\n\n    if(!is.null(results_location)){\n      results_folder_path =  file.path(results_location, paste(temp_sampleBatch,\"results\",sep = \"_\"))\n    }else{\n      results_folder_path = file.path(fcs_location, paste(temp_sampleBatch,\"results\",sep = \"_\"))\n    }\n\n    dir.create(results_folder_path, showWarnings = F)\n    setwd(results_folder_path)\n\n    summary_df = NULL\n      temp_key &lt;- subset(metadata, metadata$batch == temp_sampleBatch)\n      sample_outputPath = file.path(results_folder_path,paste0(\"flow_results_\",temp_sampleBatch))#Where I want the\n      dir.create(sample_outputPath, showWarnings = F)\n      for(i in 1:dim(temp_key)[1]){\n        temp_key[i,]$file_location &lt;- fcs_files[grepl(temp_key[i,]$file_name, fcs_files,fixed = T)]\n      }\n\n        ws = open_flowjo_xml(wsp_location)\n        comp_mat = flowjo_to_gatingset(ws,name=\"Compensation\",execute = F)[1]\n        comp_mat = gs_get_compensations(comp_mat[[1]])\n        comp_mat = as.data.frame(comp_mat[[1]]@spillover)\n        comp_mat = as.matrix(comp_mat)\n\n      for(i in 1:dim(comp_mat)[1]){\n        colnames(comp_mat)[i] = unlist(strsplit(colnames(comp_mat)[i],\" :\",fixed = T))[1]\n        rownames(comp_mat)[i] = unlist(strsplit(rownames(comp_mat)[i],\" :\",fixed = T))[1]\n      }\n\n      colnames(comp_mat) = gsub(\"Comp-\",\"\",colnames(comp_mat),fixed = T)\n      rownames(comp_mat) = gsub(\"Comp-\",\"\",rownames(comp_mat),fixed = T)\n\n      fcsFiles &lt;- temp_key$file_location\n      temp_fcs  &lt;- load_cytoset_from_fcs(fcsFiles)\n\n      temp_flowset &lt;- cytoset_to_flowSet(temp_fcs)\n\n      temp_key$name &lt;- temp_key$file_name\n\n      pData(temp_fcs)$pid &lt;- temp_sampleBatch\n\n      temp_stim_order &lt;- NULL\n      for (i in pData(temp_fcs)$name) {\n        temp_stim_order &lt;- c(temp_stim_order, subset(temp_key, temp_key$name == i)$stimulation)\n      }\n\n      pData(temp_fcs)$stimulation &lt;- temp_stim_order\n      pData(temp_fcs)$study_visit &lt;- unique(subset(temp_key, temp_key$name == i)$study_visit)\n\n      pData(temp_fcs)$control &lt;- ifelse(pData(temp_fcs)$stimulation%in%c(\"UNS\",\"unstim\",\"Unstim\"), \"TRUE\", \"FALSE\")\n\n      if(unique(grepl(\"/\",colnames(keyword(temp_fcs[[1]])$SPILL),fixed = T))){\n        colnames(comp_mat) = gsub(\"_\",\"/\",colnames(comp_mat),fixed = T)\n        rownames(comp_mat) = gsub(\"_\",\"/\",rownames(comp_mat),fixed = T)\n      }\n\n      temp_fcs &lt;- compensate(temp_fcs, comp_mat)\n\n      chnls &lt;- names(temp_fcs[[1]])\n      chnls &lt;- chnls[!grepl(\"FSC|SSC|Time\", chnls)]\n      markernames(temp_fcs) = gsub(\"IL-2\",\"IL2\",markernames(temp_fcs))\n      markernames(temp_fcs) = gsub(\"IFNg|IFN-g|IFN-y|IFNy\",\"IFNG\",markernames(temp_fcs))\n      markernames(temp_fcs) = gsub(\"TNFa|TNF-a|TNF-aphla\",\"TNF\",markernames(temp_fcs))\n      markernames(temp_fcs) = gsub(\"IL-22\",\"IL22\",markernames(temp_fcs))\n      markernames(temp_fcs) = gsub(\"IL-17\",\"IL17\",markernames(temp_fcs))\n      temp_ncfs &lt;- temp_fcs\n\n      trans &lt;- estimateLogicle(temp_ncfs[[1]], channels = chnls)\n      temp_ncfs &lt;- transform(temp_ncfs, trans)\n\n      temp_ncfs_qc = list()\n      for(i in 1:length(temp_ncfs)){\n        temp_qced = PeacoQC(temp_ncfs[[i]],channels = colnames(temp_ncfs)[colnames(temp_ncfs)!=\"Time\"],save_fcs = F,report=T,plot=T)\n        temp_ncfs_qc[[i]] &lt;- temp_ncfs[[i]][temp_qced$GoodCells, ]\n      }\n      names(temp_ncfs_qc) &lt;- rownames(pData(temp_ncfs))\n      temp_ncfs &lt;- cytoset(temp_ncfs_qc)\n\n      temp_ncfs &lt;- GatingSet(temp_ncfs)\n      gt_gating(gating_strategy, temp_ncfs)\n\n      for(i in 1:length(temp_ncfs)) {\n        temp_df = gh_pop_compare_stats(temp_ncfs[[i]])\n        temp_df$name = pData(temp_ncfs[i])$name\n        temp_df$pid = pData(temp_ncfs[i])$pid\n        temp_df$stimulation = pData(temp_ncfs[i])$stimulation\n        summary_df = rbind.data.frame(summary_df, temp_df)\n      }\n\n      gate_population_key = rbind.data.frame(\n        c(\"boundary\",\"FSC-A\",\"FSC-H\",\"cell_subsets\"),\n        c(\"singlets\",\"FSC-A\",\"FSC-H\",\"cell_subsets\"),\n        c(\"SSCneg\",\"SSC-A\",\"FSC-A\",\"cell_subsets\"),\n        c(\"lymph\",\"FSC-A\",\"SSC-A\",\"cell_subsets\"),\n        c(\"cd3.gate\",\"IFNG\",\"CD3\",\"cell_subsets\"),\n        c(\"cd8.neg\",\"CD8\",\"CD4\",\"cell_subsets\"),\n        c(\"cd4.neg\",\"CD8\",\"CD4\",\"cell_subsets\"),\n        c(\"cd4.gate\",\"CD8\",\"CD4\",\"cell_subsets\"),\n        c(\"cd8.gate\",\"CD4\",\"CD8\",\"cell_subsets\"))\n      colnames(gate_population_key) = c(\"gate\",\"xlabel\",\"ylabel\",\"summary_plot\")\n\n      if(sum(temp_gating$dims==\"IFNG\")==2){\n        temp_gate_pop = rbind.data.frame(c(\"cd4IFNG\",\"CD4\",\"IFNG\",\"cd4_t_cells\"),\n                                         c(\"cd8IFNG\",\"CD8\",\"IFNG\",\"cd8_t_cells\"))\n        colnames(temp_gate_pop) = c(\"gate\",\"xlabel\",\"ylabel\",\"summary_plot\")\n        gate_population_key = rbind.data.frame(gate_population_key,temp_gate_pop)#Using CD4 and CD8 as x-axis because I would not know what the cytokine combo\n      }\n\n      if(sum(temp_gating$dims==\"TNF\")==2){\n        temp_gate_pop = rbind.data.frame(c(\"cd4TNF\",\"CD4\",\"TNF\",\"cd4_t_cells\"),\n                                         c(\"cd8TNF\",\"CD8\",\"TNF\",\"cd8_t_cells\"))\n        colnames(temp_gate_pop) = c(\"gate\",\"xlabel\",\"ylabel\",\"summary_plot\")\n        gate_population_key = rbind.data.frame(gate_population_key,temp_gate_pop)\n      }\n\n      if(sum(temp_gating$dims==\"IL2\")==2){\n        temp_gate_pop = rbind.data.frame(c(\"cd4IL2\",\"CD4\",\"IL2\",\"cd4_t_cells\"),\n                                         c(\"cd8IL2\",\"CD8\",\"IL2\",\"cd8_t_cells\"))\n        colnames(temp_gate_pop) = c(\"gate\",\"xlabel\",\"ylabel\",\"summary_plot\")\n        gate_population_key = rbind.data.frame(gate_population_key,temp_gate_pop)\n      }\n\n      if(sum(temp_gating$dims==\"IL17\")==2){\n        temp_gate_pop = rbind.data.frame(c(\"cd4IL17\",\"CD4\",\"IL17\",\"cd4_t_cells\"),\n                                         c(\"cd8IL17\",\"CD8\",\"IL17\",\"cd8_t_cells\"))\n        colnames(temp_gate_pop) = c(\"gate\",\"xlabel\",\"ylabel\",\"summary_plot\")\n        gate_population_key = rbind.data.frame(gate_population_key,temp_gate_pop)\n      }\n\n      if(sum(temp_gating$dims==\"IL22\")==2){\n        temp_gate_pop = rbind.data.frame(c(\"cd4IL22\",\"CD4\",\"IL22\",\"cd4_t_cells\"),\n                                         c(\"cd8IL22\",\"CD8\",\"IL22\",\"cd8_t_cells\"))\n        colnames(temp_gate_pop) = c(\"gate\",\"xlabel\",\"ylabel\",\"summary_plot\")\n        gate_population_key = rbind.data.frame(gate_population_key,temp_gate_pop)\n      }\n\n      gatesToVisualize = gate_population_key$gate\n\n      for (i in 1:length(temp_ncfs)) {\n        outputPath1 = file.path(sample_outputPath, paste0(\"Flowplots_\", pData(temp_ncfs[[i]])$pid, \"_\", pData(temp_ncfs[[i]])$stimulation))\n        dir.create(outputPath1, showWarnings = F)\n        for (i_1 in 1:length(gatesToVisualize)) {\n          temp_gate = gate_population_key[i_1,]$gate\n\n          Temp.png.file.name = paste(pData(temp_ncfs[[i]])$pid,\"_\",pData(temp_ncfs[[i]])$stimulation,\"_\",gsub(\"/\",\"_\",temp_gate,fixed = T),\".png\",sep = \"\")\n          Temp.png.file.name = gsub(\"/\",\"_\",Temp.png.file.name)\n          png(filename = paste(outputPath1,\"/\",Temp.png.file.name,sep = \"\"))\n\n          if(temp_gate%in%c(\"boundary\",\"singlets\",\"SSCneg\",\"lymph\")){\n            print(autoplot(temp_ncfs[[i]],bins=180,gate=temp_gate,x=subset(gate_population_key,gate_population_key$gate==temp_gate)$xlabel,y=subset(gate_population_key,gate_population_key$gate==temp_gate)$ylabel) + geom_density2d(colour = \"black\")+\n\n                    theme(\n                      panel.background = element_rect(fill = \"white\",\n                                                      colour = \"black\",\n                                                      linewidth = 0.5, linetype = \"solid\"),\n                      panel.grid.major = element_line(linewidth = 0.5, linetype = 'solid',\n                                                      colour = \"white\"),\n                      panel.grid.minor = element_line(linewidth = 0.25, linetype = 'solid',\n                                                      colour = \"white\")\n                    ))\n          }else{\n            print(autoplot(temp_ncfs[[i]],bins=180,gate=temp_gate,x=subset(gate_population_key,gate_population_key$gate==temp_gate)$xlabel,y=subset(gate_population_key,gate_population_key$gate==temp_gate)$ylabel) + geom_density2d(colour = \"black\")+\n                    ggcyto_par_set(limits = list(x=c(-0.5,5),y=c(-0.5,5)))+\n                    theme(\n                      panel.background = element_rect(fill = \"white\",\n                                                      colour = \"black\",\n                                                      linewidth = 0.5, linetype = \"solid\"),\n                      panel.grid.major = element_line(linewidth = 0.5, linetype = 'solid',\n                                                      colour = \"white\"),\n                      panel.grid.minor = element_line(linewidth = 0.25, linetype = 'solid',\n                                                      colour = \"white\")\n                    ))\n          }\n\n          dev.off()\n        }\n      }\n\n      for (i in 1:length(temp_ncfs)) {\n        outputPath3 = file.path(sample_outputPath, paste0(\"Summary_plots_\", pData(temp_ncfs[[i]])$pid, \"_\", pData(temp_ncfs[[i]])$stimulation))\n        dir.create(outputPath3, showWarnings = F)\n        j=1\n        for(i_2 in unique(gate_population_key$summary_plot)){\n          temp_plots = subset(gate_population_key,gate_population_key$summary_plot==i_2)\n          png(filename = paste(outputPath3,\"/\",j,\"_\",i_2, \"_GatingStrategy.png\",sep = \"\"),width = 1200,height = 900)\n          temp_folder = file.path(sample_outputPath,paste0(\"Flowplots_\", pData(temp_ncfs[[i]])$pid, \"_\", pData(temp_ncfs[[i]])$stimulation))\n          temp_folder = list.files(path=temp_folder,full.names = T)\n          if(i_2==\"cell_subsets\"){\n            par(mfrow=c(3,3))\n          }else{\n            par(mfrow=c(2,3))\n          }\n\n          gates_for_strategy = subset(gate_population_key,gate_population_key$summary_plot==i_2)\n\n          for(i_1 in gates_for_strategy$gate){\n            if(!is.na(i_1)){\n\n              temp.img = image_read(temp_folder[grepl(paste(pData(temp_ncfs[i])$stimulation,\"_\",gsub(\"/\",\"_\",i_1,fixed = T),\".png\",sep = \"\"),temp_folder)],depth = 16)\n              par(mar=c(3,2,0,0))\n              par(xpd=NA)\n              plot(temp.img)\n              mtext(side = 1,subset(gates_for_strategy,gates_for_strategy$gate==i_1)$xlab,cex=2)\n              mtext(side = 2,subset(gates_for_strategy,gates_for_strategy$gate==i_1)$ylab,cex=2)\n            }else{\n              par(mar=c(0,0,0,0))\n              par(xpd=NA)\n              plot.new()\n            }\n          }\n          dev.off()\n          j=j+1\n        }\n      }\n\n      gs_cleanup_temp(temp_ncfs)\n      setwd(results_folder_path)\n      write.csv(summary_df, paste(temp_sampleBatch,\"Results_DF.csv\",sep = \"_\"),row.names = FALSE)\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automated gating of flow Cytometry Data in R</span>"
    ]
  },
  {
    "objectID": "recordings.html",
    "href": "recordings.html",
    "title": "12  Session Recordings",
    "section": "",
    "text": "12.1 Description\nOn this page you will find links to the Microsoft Teams recordings for each session. You will only be able to access these recordings if you already have institutional access via Teams. If you would like to access the recordings, but do not have access, please email the course instructors.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session Recordings</span>"
    ]
  },
  {
    "objectID": "recordings.html#session-recordings",
    "href": "recordings.html#session-recordings",
    "title": "12  Session Recordings",
    "section": "12.2 2024 Session Recordings",
    "text": "12.2 2024 Session Recordings\n\n12.2.1 Session 1: Intro to R and swirl\n05 MAR 2024\nSession 1 Video Recording\n\n\n12.2.2 Session 2: MaRcus Training Course lesson 1\n19 MAR 2024\nSession 2 Video Recording\n\n\n12.2.3 Session 3: MaRcus Training Course lesson 2\n26 MAR 2024\nSession 3 Video Recording\n\n\n12.2.4 Session 4: MaRcus Training Course lesson 3\n02 APR 2024\nSession 4 Video Recording\nNote - MaRcus Training Course lesson 4 was skipped as it covers R Markdown which will be replaced by a session on Quarto later.\n\n\n12.2.5 Session 5: MaRcus Training Course lesson 5\n09 APR 2024\nSession 5 Video Recording\n\n\n12.2.6 Session 6: MaRcus Training Course lesson 6\n30 APR 2024\nSession 6 Video Recording\n\n\n12.2.7 Session 7: MaRcus Training Course lesson 7\n07 MAY 2024\nSession 7 Video Recording\n\n\n12.2.8 Session 8: Exporting data from R\n21 MAY 2024\nSession 8 Video Recording",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session Recordings</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Summary",
    "section": "",
    "text": "We hope you have enjoyed the course! Remember that the best way to learn how to code is to experiment and use coding languages as much as possible. Just like learning a foreign language, learning to code works best with daily practice.\nIf you have any questions, feedback, or suggestions, please contact the course instructors.\nBest of luck on your computational journey!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "drafts.html",
    "href": "drafts.html",
    "title": "14  Drafts",
    "section": "",
    "text": "All the following pages are drafts. They may ultimately be removed from the website and are likely not in their final state, but they are still available for viewing.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Drafts</span>"
    ]
  },
  {
    "objectID": "draft-targets.html",
    "href": "draft-targets.html",
    "title": "15  Reproducible Pipelines with targets",
    "section": "",
    "text": "16 Introduction to targets\nIn data analysis projects, ensuring reproducibility is crucial. The targets package in R provides a powerful framework for building reproducible, scalable, and efficient data pipelines. This tutorial will guide you through the basics of using targets to manage your analysis workflow.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "draft-targets.html#purpose",
    "href": "draft-targets.html#purpose",
    "title": "15  Reproducible Pipelines with targets",
    "section": "16.1 Purpose",
    "text": "16.1 Purpose\nThe primary purpose of using targets is to prevent rerunning parts of the pipeline if their dependencies haven’t changed. This optimization saves time and computational resources by only executing the necessary steps when data or code modifications occur.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "draft-targets.html#getting-started",
    "href": "draft-targets.html#getting-started",
    "title": "15  Reproducible Pipelines with targets",
    "section": "16.2 Getting Started",
    "text": "16.2 Getting Started\n\n16.2.1 Installation\nFirst, install the targets package from CRAN if you haven’t already:\n\n\nCode\ninstall.packages(\"targets\")\n#&gt; Installing package into '/workspaces/.local/lib/R'\n#&gt; (as 'lib' is unspecified)\nlibrary(targets)\n\n\n\n\n16.2.2 Initialization\nLoad the targets package and initialize a new targets pipeline in your project directory:\n\n\nCode\nlibrary(targets)\nuse_targets()\n\n\nThis command creates a _targets.R script, with helpful setup instructions.\nHere is a slightly reduced version of it:\n# Created by use_targets().\n# Follow the comments below to fill in this target script.\n# Then follow the manual to check and run the pipeline:\n#   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline\n\n# Load packages required to define the pipeline:\nlibrary(targets)\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\") # Packages that your targets need for their tasks.\n)\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n\n# tar_source(\"other_functions.R\") # Source other scripts as needed.\n\n# Replace the target list below with your own:\nlist(\n  tar_target(\n    name = data,\n    command = tibble(x = rnorm(1e4), y = rnorm(1e4))\n    # format = \"qs\" # Efficient storage for general data objects.\n  ),\n  tar_target(\n    name = model,\n    command = coefficients(lm(y ~ x, data = data))\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "draft-targets.html#running-the-pipeline",
    "href": "draft-targets.html#running-the-pipeline",
    "title": "15  Reproducible Pipelines with targets",
    "section": "16.3 Running the pipeline",
    "text": "16.3 Running the pipeline\nWe then run this pipeline using tar_make(). We do that below, and time how long the pipeline takes to execute:\n\n\nCode\ntime_start &lt;- proc.time()[3]\ntar_make()\n#&gt; - One or more packages recorded in the lockfile are not installed.\n#&gt; - Use `renv::status()` for more details.\n#&gt; tar_source(): these files do not exist: R\n#&gt; ✔ skipped target data\n#&gt; ✔ skipped target model\n#&gt; ✔ skipped pipeline [0.068 seconds]\ntime_end &lt;- proc.time()[3]\n(time_end - time_start) |&gt; signif(2)\n#&gt; elapsed \n#&gt;     1.3\n\n\nNow, we can rerun it, and it will take less time:\n\n\nCode\ntime_start &lt;- proc.time()[3]\ntar_make()\n#&gt; - One or more packages recorded in the lockfile are not installed.\n#&gt; - Use `renv::status()` for more details.\n#&gt; tar_source(): these files do not exist: R\n#&gt; ✔ skipped target data\n#&gt; ✔ skipped target model\n#&gt; ✔ skipped pipeline [0.069 seconds]\ntime_end &lt;- proc.time()[3]\n(time_end - time_start) |&gt; signif(2)\n#&gt; elapsed \n#&gt;     1.3\n\n\nIt doesn’t drive it down to zero as targets has some overhead (e.g., checking dependencies), but it’s obviously faster. This difference becomes dramatic when slow-running steps are involved.\nIf we adjust the pipeline, then tar_make() will rerun what is required:\n\n\nCode\n# Read the file\nfile_path &lt;- \"_targets.R\"\nfile_content &lt;- readLines(file_path)\n\n# Replace the specific line\nfile_content &lt;- gsub(\"y ~ x\", \"y ~ x - 1\", file_content)\n\n# Write the updated content back to the file\nwriteLines(file_content, file_path)\n\n# Run the pipeline\ntime_start &lt;- proc.time()[3]\ntar_make()\n#&gt; - One or more packages recorded in the lockfile are not installed.\n#&gt; - Use `renv::status()` for more details.\n#&gt; tar_source(): these files do not exist: R\n#&gt; ✔ skipped target data\n#&gt; ▶ dispatched target model\n#&gt; ● completed target model [0.264 seconds, 83 bytes]\n#&gt; ▶ ended pipeline [0.434 seconds]\ntime_end &lt;- proc.time()[3]\n(time_end - time_start) |&gt; signif(2)\n#&gt; elapsed \n#&gt;     1.7\n\n\nThe time is still faster than before, as we didn’t simulate the data, but it’s slower than when the model fit was cached.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "draft-targets.html#accessing-outputs",
    "href": "draft-targets.html#accessing-outputs",
    "title": "15  Reproducible Pipelines with targets",
    "section": "16.4 Accessing Outputs",
    "text": "16.4 Accessing Outputs\nAfter running the pipeline, you can access the outputs of the targets using:\n\ntar_load(&lt;name&gt;): Loads the target object into the R environment.\n\n\nCode\ntar_load(model)\nsummary(model)\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; -0.002124 -0.002124 -0.002124 -0.002124 -0.002124 -0.002124\n\n\ntar_read(&lt;name&gt;): Reads the target object without loading it into the environment.\n\n\nCode\nmodel_summary &lt;- tar_read(model)\nsummary(model_summary)\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; -0.002124 -0.002124 -0.002124 -0.002124 -0.002124 -0.002124",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "draft-targets.html#debugging-the-pipeline",
    "href": "draft-targets.html#debugging-the-pipeline",
    "title": "15  Reproducible Pipelines with targets",
    "section": "16.5 Debugging the Pipeline",
    "text": "16.5 Debugging the Pipeline\nDebugging is an essential part of developing a targets pipeline. However, you cannot directly debug a targets pipeline by default (it runs the code “separately” from the R console).\nHere are two methods to debug your pipeline.\n\n16.5.1 Option 1: Using browser()\nInsert browser() into the function where you want to inspect the pipeline’s state.\n\n\nCode\ntar_target(\n  model,\n  {\n    browser()\n    lm_model &lt;- lm(value_log ~ predictor, data = data_processed)\n    lm_model\n  }\n)\n\n\nRun the pipeline with:\n\n\nCode\ntargets::tar_make(callr_function = NULL)\n#&gt; tar_source(): these files do not exist: R\n#&gt; Warning: Targets and globals must have unique names. Ignoring global objects\n#&gt; that conflict with target names: model. Warnings like this one are important,\n#&gt; but if you must suppress them, you can do so with Sys.setenv(TAR_WARN =\n#&gt; \"false\").\n#&gt; ✔ skipped target data\n#&gt; ▶ dispatched target model\n#&gt; ● completed target model [0.39 seconds, 107 bytes]\n#&gt; ▶ ended pipeline [0.588 seconds]\n\n\nThis allows you to interactively debug the target execution.\n\n\n16.5.2 Option 2: Loading Everything\nLoad all targets into the environment to inspect and interact with them directly.\n\n\nCode\ntargets::tar_load_everything()\n\n\nThis approach is useful for exploratory debugging and inspecting intermediate results.\nFor example, after running targets::tar_load_everything(), you can access the data object directly. You can then play around with the code to fit the model, having the data at hand.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "draft-targets.html#more-niche-topics",
    "href": "draft-targets.html#more-niche-topics",
    "title": "15  Reproducible Pipelines with targets",
    "section": "16.6 More niche topics",
    "text": "16.6 More niche topics\n\n16.6.1 Multiple pipelines\nWithin a given project, one may define multiple pipelines.\n\n\n16.6.2 Forced Execution\nSometimes, you may want a target to always rerun, regardless of whether its dependencies have changed. Use tar_cue(cue = \"always\") to enforce this behavior.\n\n\nCode\ntar_target(\n  figure,\n  {\n    plot &lt;- ggplot(data_processed, aes(x = predictor, y = value_log)) +\n      geom_point() +\n      geom_smooth(method = \"lm\") +\n      theme_minimal()\n    ggsave(\"figures/model_plot.png\", plot)\n    plot\n  },\n  cue = tar_cue(\"always\")\n)\n\n\nThis ensures that the figure target is always executed when tar_make() is run.\nSometimes you just need to do this, e.g. targets can’t pick up that the data has changed or the output has been wiped in between runs.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "draft-targets.html#further-resources",
    "href": "draft-targets.html#further-resources",
    "title": "15  Reproducible Pipelines with targets",
    "section": "16.7 Further Resources",
    "text": "16.7 Further Resources\n\nHome Page: Dynamic Function-Oriented Make-Like Declarative Pipelines • targets\nDetailed User Manual: The {targets} R package user manual",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reproducible Pipelines with `targets`</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]